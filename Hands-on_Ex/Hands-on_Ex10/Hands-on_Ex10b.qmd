---
title: "Hands On Exercise 10 Part 2: Calibrating Spatial Interaction Models with R"
author: "Arjun Singh"
date: 2024-10-31
date-modified: "last-modified"
execute: 
  eval: true
  echo: true
  message: false
  freeze: true
  warning: false
format:
  html:
    css: styles.css 
---

# 10 Introduction

Spatial Interaction Models (SIMs) are mathematical frameworks designed to estimate the flows or movements between spatial entities, developed by Alan Wilson in the late 1960s and early 1970s. Since then, these models have become fundamental tools in transport modeling and have undergone extensive refinement, as highlighted by Boyce and Williams (2015).

The four main types of traditional SIMs, as defined by Wilson (1971), include:

1.  **Unconstrained Models**: These models do not impose any restrictions on the origin or destination totals, providing a basic estimation of flows based purely on the distance or accessibility between locations.
2.  **Production-Constrained Models**: These models fix the total flow originating from each location, allowing each origin to have a specified volume of flows but leaving the destination totals unconstrained.
3.  **Attraction-Constrained Models**: These models constrain the flow arriving at each destination, allowing for control over destination totals while leaving origin totals unconstrained.
4.  **Doubly-Constrained Models**: These models constrain both the origin and destination totals, balancing flows so that the volume leaving each origin matches the specified amount, as does the volume arriving at each destination.

To calibrate these models and accurately predict flows, various regression methods have been applied, such as:

-   **Ordinary Least Squares (OLS)**: Assumes a linear relationship and minimizes the sum of squared differences between observed and predicted flows.
-   **Log-Normal Regression**: Useful when flow data exhibit log-normal distribution characteristics, handling right-skewed data often seen in flow volumes.
-   **Poisson Regression**: Suitable for modeling count data, especially when flows represent non-negative integers like the number of trips or shipments.
-   **Negative Binomial (NB) Regression**: Often used when flow data are over-dispersed (variance exceeds the mean), providing a more flexible fit than Poisson regression.

In this exercise, we work with these regression methods to calibrate SIMs in R. We use specialized R packages to apply each method, gaining practical experience in selecting and implementing the appropriate calibration techniques for OD flow data.

::: callout-note
Calibration is the process of adjusting parameters in the model to try and get the estimates to agree with the observed data as much as possible. Adjusting the parameters is the sort of iterative process that computers are particularly good at and the goodness-of-fit statistics can be used to indicate when the optimum solution is found. Historically this process required a researcher with the requisite programming skills to write a computer algorithm to iteratively adjust each parameter, check the goodness-of-fit, and then start all over again until the goodness-of-fit statistic was maximised/minimised. (Adam Dennett, 2018)
:::

# 10.3 Data and Packages

In this exercise, we are going to calibrate SIM to determine factors affecting the public bus passenger flows during the morning peak in Singapore.

This exercise is a continuation of **Chapter 15: Processing and Visualising Flow Data** and the following data will be used:

-   *od_data.rds*, weekday morning peak passenger flows at planning subzone level.
-   *mpsz.rds*, URA Master Plan 2019 Planning Subzone boundary in simple feature tibble data frame format.

Beside these two data sets, an additional attribute data file called pop.csv will be used.

We use the following 4 R packages:

-   `sf` for importing, integrating, processing and transforming geospatial data.
-   `tidyverse` for importing, integrating, wrangling and visualising data.
-   `tmap` for creating thematic maps.

We load the packages into our environment by using the p_load() function of the pacman package.

```{r}
pacman::p_load(tmap, sf, sp,
               performance, reshape2,
               ggpubr, tidyverse)
set.seed(1234)
```

## 10.3.1 Computing Distance Matrix

In spatial interaction, a distance matrix is a table that shows the distance between pairs of locations. For example, in the table below we can see an Euclidean distance of 3926.0025 between MESZ01 and RVSZ05, of 3939.1079 between MESZ01 and SRSZ01, and so on. By definition, an location's distance from itself, which is shown in the main diagonal of the table, is 0.

![](images/clipboard-1475824881.png)

In this section, you will learn how to compute a distance matrix by using URA Master Plan 2019 Planning Subzone boundary in which you saved as an rds file called *mpsz*.

We first import *mpsz.rds* into our environment by using the code chunk below.

```{r}
mpsz <- read_rds("data/rds/mpsz.rds")
mpsz
```

::: note-box
Notice that it is a sf tibble dataframe object class.
:::

### 10.3.2 Converting from sf data.table to SpatialPolygonsDataFrame

There are at least two ways to compute the required distance matrix. One is based on sf and the other is based on sp. Past experience shown that computing distance matrix by using sf function took relatively longer time that sp method especially the data set is large. In view of this, sp method is used in the code chunks below.

First [`as.Spatial()`](https://r-spatial.github.io/sf/reference/coerce-methods.html) will be used to convert *mpsz* from sf tibble data frame to SpatialPolygonsDataFrame of sp object as shown in the code chunk below.

```{r}
mpsz_sp <- as(mpsz, "Spatial")
mpsz_sp
```

### 10.3.3 Computing the distance matrix

We now use the [`spDists()`](https://www.rdocumentation.org/packages/sp/versions/2.1-1/topics/spDistsN1) function of the sp package to compute the Euclidean distance between the centroids of the planning subzones.

::: insights-box
The distance between two centroids of a pair of spatial polygons is calculated to provide a representative measure of the spatial separation between those areas. Here’s why centroids are typically used for this purpose:

1.  **Simplification of Complex Shapes**: Spatial polygons, such as cities, neighborhoods, or districts, can have irregular and complex shapes. Calculating the distance between every point in each polygon would be computationally intensive and impractical. Instead, centroids serve as single, representative points, allowing for efficient and consistent distance calculations.

2.  **Average Spatial Location**: A centroid represents the "center of mass" or the geometric center of a polygon, making it a good approximation of the area’s average spatial location. This helps when comparing distances across multiple polygons, as centroids offer a consistent reference point that reflects the overall position of each area.

3.  **Consistency in Spatial Interaction Models**: In models like Spatial Interaction Models (SIMs), distance is a critical variable affecting flow or interaction. By using centroids, researchers can systematically calculate and compare distances between origins and destinations without bias toward specific edge points or local variations within each polygon.

4.  **Efficient for Travel and Transport Models**: In many applications, such as travel or transport modeling, the centroid-based distance serves as a reasonable approximation of actual travel distance. Although not as precise as route-based distances, it provides a standardized measure that is often close to real-world travel patterns, particularly for large polygons like cities or regions.

In essence, using centroids simplifies calculations while providing a meaningful and comparable measure of distance between spatial polygons. This approach enables models to predict interaction patterns effectively and is widely accepted in spatial analysis, transport modeling, and urban planning.
:::

::: panel-tabset
## Computation

```{r}
dist <- spDists(mpsz_sp, 
                longlat = FALSE)
```

## Viewing the Data

```{r}
head(dist, n=c(10, 10))
```
:::

::: note-bpx
Notice that the output *dist* is a matrix object class of R. Also notice that the column heanders and row headers are not labeled with the planning subzone codes.
:::

### 10.3.4 Labelling column and row heanders of a distance matrix

First, we will create a list sorted according to the the distance matrix by planning sub-zone code.

```{r}
sz_names <- mpsz$SUBZONE_C
```

Next we will attach `SUBZONE_C` to row and column for distance matrix matching ahead

```{r}
colnames(dist) <- paste0(sz_names)
rownames(dist) <- paste0(sz_names)
```

### Pivoting distance value by SUBZONE_C

Next, we will pivot the distance matrix into a long table by using the row and column subzone codes as show in the code chunk below.

```{r}
distPair <- melt(dist) %>%
  rename(dist = value)
head(distPair, 10)
```

Notice that the within zone distance is 0.

### Updating intra-zonal distances

::: panel-tabset
## Find minimum distance

We now append a constant value to replace the intra-zonal distance of 0.

First, we will select and find out the minimum value of the distance by using `summary()`.

```{r}
distPair %>%
  filter(dist > 0) %>%
  summary()
```

## Add constant Distance Value

Next, a constant distance value of 50m is added into intra-zones distance.

```{r}
distPair$dist <- ifelse(distPair$dist == 0,
                        50, distPair$dist)
```

## Summary of resulting data-frame

The code chunk below will be used to check the resulting data-frame.

```{r}
distPair %>%
  summary()
```

## Rename data fields

The code chunk below is used to rename the origin and destination fields.

```{r}
#| eval: false
distPair <- distPair %>%
  rename(orig = Var1,
         dest = Var2)
```

## RDS File

Lastly, the code chunk below is used to save the dataframe for future use.

```{r}
#| eval: false
write_rds(distPair, "data/rds/distPair.rds") 
```

```{r}
distPair <- read_rds("data/rds/distPair.rds")
```
:::

## 10.3.2 Preparing flow data

The code chunk below is used import *od_data* into our environment.

```{r}
od_data_fii <- read_rds("data/rds/od_data_fii.rds")
```

We now compute the total passenger trips between and within planning subzones by using the code chunk below.

::: panel-tabset
## Computation

The output is all *flow_data*.

```{r}
flow_data <- od_data_fii %>%
  group_by(ORIGIN_SZ, DESTIN_SZ) %>% 
  summarize(TRIPS = sum(MORNING_PEAK)) 
```

## View the data

Use the code chunk below to display flow_data dataframe.

```{r}
head(flow_data, 10)
```
:::

### 10.3.2.1 Separating intra-flow from passenger volume df

The code chunk below is used to add three new fields in `flow_data` dataframe.

```{r}
flow_data$FlowNoIntra <- ifelse(
  flow_data$ORIGIN_SZ == flow_data$DESTIN_SZ, 
  0, flow_data$TRIPS)
flow_data$offset <- ifelse(
  flow_data$ORIGIN_SZ == flow_data$DESTIN_SZ, 
  0.000001, 1)
```

### 10.3.2.2 Combining passenger volume data with distance value

Before we can join *flow_data* and *distPair*, we must convert the data value type of *ORIGIN_SZ* and *DESTIN_SZ* fields of flow_data data-frame into factor data type.

```{r}
flow_data$ORIGIN_SZ <- as.factor(flow_data$ORIGIN_SZ)
flow_data$DESTIN_SZ <- as.factor(flow_data$DESTIN_SZ)
```

We now implement the `left_join()` function of the **dplyr** package to join the *flow_data* dataframe and *distPair* dataframe. The output is called *flow_data1*.

```{r}
flow_data1 <- flow_data %>%
  left_join (distPair,
             by = c("ORIGIN_SZ" = "orig",
                    "DESTIN_SZ" = "dest"))
```

## 10.3.3 Preparing Origin and Destination Attributes

### 10.3.3.1 Importing population data

```{r}
pop <- read_csv("data/aspatial/pop.csv")
```

### 10.3.3.2 Geospatial data wrangling

```{r}
#| eval: false
pop <- pop %>%
  left_join(mpsz,
            by = c("PA" = "PLN_AREA_N",
                   "SZ" = "SUBZONE_N")) %>%
  select(1:6) %>%
  rename(SZ_NAME = SZ,
         SZ = SUBZONE_C)
```

### 10.3.3.3 Preparing origin attribute

```{r}
#| eval: false
flow_data1 <- flow_data1 %>%
  left_join(pop,
            by = c(ORIGIN_SZ = "SZ")) %>%
  rename(ORIGIN_AGE7_12 = AGE7_12,
         ORIGIN_AGE13_24 = AGE13_24,
         ORIGIN_AGE25_64 = AGE25_64) %>%
  select(-c(PA, SZ_NAME))

```

### 10.3.3.4 Preparing destination attribute

```{r}
#| eval: false
flow_data1 <- flow_data1 %>%
  left_join(pop,
            by = c(DESTIN_SZ = "SZ")) %>%
  rename(DESTIN_AGE7_12 = AGE7_12,
         DESTIN_AGE13_24 = AGE13_24,
         DESTIN_AGE25_64 = AGE25_64) %>%
  select(-c(PA, SZ_NAME))
```

We will called the output data file *SIM_data*. it is in rds data file format.

::: panel-tabset
## RDS File creation

```{r}
#| eval: false
write_rds(flow_data1, "data/rds/flow_data_6-9.rds")
```

## Reading RDS File

```{r}
SIM_data <- read_rds("data/rds/flow_data_6-9.rds")

```
:::

# 10.4 Calibrating Spatial Interaction Models

We now calibrate Spatial Interaction Models by using Poisson Regression method.

## 10.4.1 Visualising the dependent variable

We first plot the distribution of the dependent variable (i.e. TRIPS) by using histogram method by using the code chunk below.

```{r}
ggplot(data = SIM_data,
       aes(x = TRIPS)) +
  geom_histogram()
```

::: note-box
Notice that the distribution is highly skewed and not resemble bell shape or also known as normal distribution.
:::

Next, let us visualise the relation between the dependent variable and one of the key independent variable in Spatial Interaction Model, namely distance.

```{r}
ggplot(data = SIM_data,
       aes(x = dist,
           y = TRIPS)) +
  geom_point() +
  geom_smooth(method = lm)
```

Notice that their relationship hardly resembles linear relationship.

On the other hand, if we plot the scatter plot by using the log transformed version of both variables, we can see that their relationship resembles linear relationship more closely.

```{r}
ggplot(data = SIM_data,
       aes(x = log(dist),
           y = log(TRIPS))) +
  geom_point() +
  geom_smooth(method = lm)
```

## 10.4.2 Checking for variables with zero values

Since Poisson Regression is based of log and log 0 is undefined, it is important for us to ensure that no 0 values in the explanatory variables.

In the code chunk below, summary() of Base R is used to compute the summary statistics of all variables in *SIM_data* data frame.

```{r}
summary(SIM_data)
```

The print report above reveals that variables ORIGIN_AGE7_12, ORIGIN_AGE13_24, ORIGIN_AGE25_64,DESTIN_AGE7_12, DESTIN_AGE13_24, DESTIN_AGE25_64 consist of 0 values.

In view of this, code chunk below will be used to replace zero values to 0.99.

```{r}
SIM_data$DESTIN_AGE7_12 <- ifelse(
  SIM_data$DESTIN_AGE7_12 == 0,
  0.99, SIM_data$DESTIN_AGE7_12)
SIM_data$DESTIN_AGE13_24 <- ifelse(
  SIM_data$DESTIN_AGE13_24 == 0,
  0.99, SIM_data$DESTIN_AGE13_24)
SIM_data$DESTIN_AGE25_64 <- ifelse(
  SIM_data$DESTIN_AGE25_64 == 0,
  0.99, SIM_data$DESTIN_AGE25_64)
SIM_data$ORIGIN_AGE7_12 <- ifelse(
  SIM_data$ORIGIN_AGE7_12 == 0,
  0.99, SIM_data$ORIGIN_AGE7_12)
SIM_data$ORIGIN_AGE13_24 <- ifelse(
  SIM_data$ORIGIN_AGE13_24 == 0,
  0.99, SIM_data$ORIGIN_AGE13_24)
SIM_data$ORIGIN_AGE25_64 <- ifelse(
  SIM_data$ORIGIN_AGE25_64 == 0,
  0.99, SIM_data$ORIGIN_AGE25_64)

```

You can run the summary() again.

```{r}
summary(SIM_data)
```

Notice that all the 0 values have been replaced by 0.99.

## 10.4.3 Unconstrained Spatial Interaction Model

In this section, you will learn how to calibrate an unconstrained spatial interaction model by using `glm()` of Base Stats. The explanatory variables are origin population by different age cohort, destination population by different age cohort (i.e. *ORIGIN_AGE25_64*) and distance between origin and destination in km (i.e. *dist*).

The general formula of Unconstrained Spatial Interaction Model is:

![](images/clipboard-2098303418.png)

The code chunk used to calibrate to model is shown below:

```{r}
uncSIM <- glm(formula = TRIPS ~ 
                log(ORIGIN_AGE25_64) + 
                log(DESTIN_AGE25_64) +
                log(dist),
              family = poisson(link = "log"),
              data = SIM_data,
              na.action = na.exclude)
uncSIM
```

## 10.4.4 R-squared function

In order to measure how much variation of the trips can be accounted by the model we write a function to calculate R-Squared value as shown below.

```{r}
CalcRSquared <- function(observed,estimated){
  r <- cor(observed,estimated)
  R2 <- r^2
  R2
}
```

We now compute the R-squared of the unconstrained SIM by using the code chunk below.

```{r}
CalcRSquared(uncSIM$data$TRIPS, uncSIM$fitted.values)
```

```{r}
r2_mcfadden(uncSIM)
```

## 10.4.5 Origin (Production) constrained SIM

In this section, we will fit an origin constrained SIM by using the code3 chunk below.

The general formula of Origin Constrained Spatial Interaction Model is:

![](images/clipboard-2418230122.png)

```{r}
orcSIM <- glm(formula = TRIPS ~ 
                 ORIGIN_SZ +
                 log(DESTIN_AGE25_64) +
                 log(dist),
              family = poisson(link = "log"),
              data = SIM_data,
              na.action = na.exclude)
summary(orcSIM)
```

We can examine how the constraints hold for destinations this time.

```{r}
CalcRSquared(orcSIM$data$TRIPS, orcSIM$fitted.values)
```

## 10.4.6 Destination constrained

In this section, we will fit a destination constrained SIM by using the code chunk below.

The general formula of Destination Constrained Spatial Interaction Model is:

![](images/clipboard-787820714.png)

```{r}
decSIM <- glm(formula = TRIPS ~ 
                DESTIN_SZ + 
                log(ORIGIN_AGE25_64) + 
                log(dist),
              family = poisson(link = "log"),
              data = SIM_data,
              na.action = na.exclude)
summary(decSIM)
```

We can examine how the constraints hold for destinations this time.

```{r}
CalcRSquared(decSIM$data$TRIPS, decSIM$fitted.values)
```

## 10.4.7 Doubly constrained

In this section, we will fit a doubly constrained SIM by using the code chunk below.

The general formula of Doubly Constrained Spatial Interaction Model is:

![](images/clipboard-1522313502.png)

```{r}
dbcSIM <- glm(formula = TRIPS ~ 
                ORIGIN_SZ + 
                DESTIN_SZ + 
                log(dist),
              family = poisson(link = "log"),
              data = SIM_data,
              na.action = na.exclude)
summary(dbcSIM)
```

We can examine how the constraints hold for destinations this time.

```{r}
CalcRSquared(dbcSIM$data$TRIPS, dbcSIM$fitted.values)
```

Notice that there is a relatively greater improvement in the R\^2 value.

## 10.4.8 Model comparison

Another useful model performance measure for continuous dependent variable is [Root Mean Squared Error](https://towardsdatascience.com/what-does-rmse-really-mean-806b65f2e48e). In this sub-section, you will learn how to use [`compare_performance()`](https://easystats.github.io/performance/reference/compare_performance.html) of [**performance**](https://easystats.github.io/performance/index.html) package

We first create a list called *model_list* by using the code chunk below.

```{r}
model_list <- list(unconstrained=uncSIM,
                   originConstrained=orcSIM,
                   destinationConstrained=decSIM,
                   doublyConstrained=dbcSIM)
```

Next, we will compute the RMSE of all the models in *model_list* file by using the code chunk below.

```{r}
compare_performance(model_list,
                    metrics = "RMSE")
```

The print above reveals that doubly constrained SIM is the best model among all the four SIMs because it has the smallest RMSE value of 1487.111.

## 10.4.9 Visualizing fitted values

In this section, you will learn how to visualize the observed values and the fitted values.

Firstly we will extract the fitted values from each model by using the code chunk below.

```{r}
df <- as.data.frame(uncSIM$fitted.values) %>%
  round(digits = 0)
```

Next, we will join the values to *SIM_data* data frame.

```{r}
SIM_data <- SIM_data %>%
  cbind(df) %>%
  rename(uncTRIPS = "uncSIM$fitted.values")
```

Repeat the same step by for Origin Constrained SIM (i.e. orcSIM)

```{r}
df <- as.data.frame(orcSIM$fitted.values) %>%
  round(digits = 0)
```

```{r}
SIM_data <- SIM_data %>%
  cbind(df) %>%
  rename(orcTRIPS = "orcSIM$fitted.values")
```

Repeat the same step for Destination Constrained SIM (i.e. decSIM)

```{r}
df <- as.data.frame(decSIM$fitted.values) %>%
  round(digits = 0)
```

```{r}
SIM_data <- SIM_data %>%
  cbind(df) %>%
  rename(decTRIPS = "decSIM$fitted.values")
```

Repeat the same step by for Doubly Constrained SIM (i.e. dbcSIM)

```{r}
df <- as.data.frame(dbcSIM$fitted.values) %>%
  round(digits = 0)
```

```{r}
SIM_data <- SIM_data %>%
  cbind(df) %>%
  rename(dbcTRIPS = "dbcSIM$fitted.values")
```

```{r}
#| fig-height: 8
unc_p <- ggplot(data = SIM_data,
                aes(x = uncTRIPS,
                    y = TRIPS)) +
  geom_point() +
  geom_smooth(method = lm)

orc_p <- ggplot(data = SIM_data,
                aes(x = orcTRIPS,
                    y = TRIPS)) +
  geom_point() +
  geom_smooth(method = lm)

dec_p <- ggplot(data = SIM_data,
                aes(x = decTRIPS,
                    y = TRIPS)) +
  geom_point() +
  geom_smooth(method = lm)

dbc_p <- ggplot(data = SIM_data,
                aes(x = dbcTRIPS,
                    y = TRIPS)) +
  geom_point() +
  geom_smooth(method = lm)
```

Now, we will put all the graphs into a single visual for better comparison by using the code chunk below.

```{r}
#| fig-width: 12
#| fig-height: 7
ggarrange(unc_p, orc_p, dec_p, dbc_p,
          ncol = 2,
          nrow = 2)

```
