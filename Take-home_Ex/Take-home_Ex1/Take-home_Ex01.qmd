---
title: "Take Home Exercise 1"
author: "Arjun Singh"
date: 2024-09-02
date-modified: "last-modified"
execute: 
  eval: true
  echo: true
  message: false
  freeze: true
  warning: false
format:
  html:
    css: styles.css  
---

# 1 Introduction

According to the World Health Organisation (WHO), road traffic accidents cause approximately 1.19 million deaths annually and result in between 20 and 50 million people with being down with non-fatal injuries. Over half of all road traffic deaths occur among vulnerable road users, such as pedestrians, cyclists and motorcyclists.

Road traffic injuries are the leading cause of death for children and young adults aged 5–29 while two-thirds of road traffic fatalities occur among people of working age (18–59 years). 9 in 10 fatalities on the roads occur in low and middle-income countries, even though these countries have only around 60% of the world’s vehicles.

In addition to the human suffering caused by road traffic injuries, they also inflict a heavy economic burden on victims and their families, both through treatment costs for the injured and through loss of productivity of those killed or disabled. More broadly, road traffic injuries have a serious impact on national economies, costing countries 3% of their annual GDP.

Thailand’s roads are the deadliest in Southeast Asia and among the worst in the world, according to the World Health Organisation. About 20,000 people die in road accidents each year, or about 56 deaths a day (WHO).

Between 2014 and 2021, Thailand experienced a notable increase in accidents. Specifically, 19% of all accidents in Thailand occurred on the national highways, which constituted the primary public thoroughfares connecting various regions, provinces, districts, and significant locations within a comprehensive network.

Within the broader context of accidents across the country, there existed a considerable 66% likelihood of encountering accident-prone zones, often termed ‘**black spots**,’ distributed as follows: 66% on straight road segments, 13% at curves, 6% at median points of cross-shaped intersections, 5% at T-shaped intersections and Y-shaped intersections, 3% at cross-shaped intersections, 2% on bridges, and 2% on steep slopes, respectively.

# 1.1 Objectives

For the most part, road accidents can be attributed to two key factors- behavioral factors. such as driving skill, and environmental factors, such as heavy rain.

For this exercise, we will implement Spatial and Spatio-Temporal Point Pattern Analysis methods to discover the underlying factors behind the accidents in the Bangkok Metropolitan Region.

Over the course of this analysis, we carry out the following: First Order Spatial Point Pattern Analysis, Second Order Spatial Point Pattern Analysis, Network Constrained Spatial Point Pattern Analysis, Spatial Autocorrelation amongst others.

# 1.2 Data and Packages

For the purpose of this study, we will use the following three data-frames.

-   [Thailand Road Accidents \[2019-2022\]](https://www.kaggle.com/datasets/thaweewatboy/thailand-road-accident-2019-2022), sourced from Kaggle.

-   [Thailand Roads Open Street Map Export](https://data.humdata.org/dataset/hotosm_tha_roads), sourced from Humanitarian Data Exchange.

-   [Thailand Subnational Administrative Boundaries](https://data.humdata.org/dataset/cod-ab-tha?) sourced from Humanitarian Data Exchange.

You can click the link embedded on each of these data-frames to learn more.

We will use the following R packages for our analysis:

-   `sf`, a relatively new R package specially designed to import, manage and process vector-based geospatial data in R.

-   `spatstat`, which has a wide range of useful functions for point pattern analysis. In this hands-on exercise, it will be used to perform 1st- and 2nd-order spatial point patterns analysis and derive kernel density estimation (KDE) layer.

-   `raster`, which reads, writes, manipulates, analyses and model of gridded spatial data (i.e. raster). In this hands-on exercise, it will be used to convert image output generate by spatstat into raster format.

-   `tidyverse` simplifies spatial analysis by offering a consistent and efficient framework that facilitates working with spatial data.

-   `tmap` which provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API.

-   `spNetwork` which provides functions to perform Spatial Point Patterns Analysis such as kernel density estimation (KDE) and K-function on network. It also can be used to build spatial matrices (‘listw’ objects like in ‘spdep’ package) to conduct any kind of traditional spatial analysis with spatial weights based on reticular distances.

-   **`spdep`**: A package for spatial dependence and spatial regression analysis, particularly for handling spatial weights.

-   **`knitr`**: A dynamic report generation tool in R, allowing for the integration of code, results, and narrative in reproducible documents.

We import them into our environment using the code chunk below. The **p_load**() function of the **pacman** package is used for this.

<details>

<summary>Click to show/hide code chunk</summary>

```{r}
pacman::p_load(sf, spatstat, raster, tidyverse, tmap, spNetwork, spdep, knitr) 
set.seed(123)

```

</details>

We will now import the data-frames mentioned above into our environment. We save them as RDS files in the interest of computational efficiency.

## 1.2.1 Aspatial Data

We first import in the data-frame contained data of all accidents in the Bangkok Metropolitan Region. For this, we implement the read_rds() function as shown in the code chunk below.

```{r}
rdacc_sf=read_rds("data/rds/acc_sf")%>% 
  mutate(hourofday= hour(incident_datetime))%>%
  mutate(traffic_period = case_when(
    # Define peak hours: 7-9 AM and 4-7 PM (16-19 in 24-hour format)
    hourofday >= 7 & hourofday <= 9 ~ "Peak",
    hourofday >= 16 & hourofday <= 19 ~ "Peak",
    TRUE ~ "Off-Peak"  # Everything else is off-peak
  ))
st_crs(rdacc_sf)
```

## 1.2.2 Geospatial Data

We import the OpenStreetMap export of Thailand into our environment using the st_read() function.

```{r}
#| eval: false
road_sf <- st_read(dsn = "data/rawdata", layer="hotosm_tha_roads_lines_shp")



```

::: note-box
It is important to note the details shown in the output above. Most important takeaway from the above output is that there is no CRS information. This information is crucial as it informs our data preparation steps. We must ensure that the right CRS information is set in order to carry out analysis.
:::

Below, we import the boundary data. `map_sf` is the boundary data at the province level (ADM1), while `map2_sf` is the boundary data at the district level (ADM2). We will be using both over the course of our analysis.

```{r}
map_sf <- st_read(dsn = "data/rawdata", layer="tha_admbnda_adm1_rtsd_20220121")
```

```{r}
map2_sf <- st_read(dsn = "data/rawdata", layer="tha_admbnda_adm2_rtsd_20220121")
```

After importing the boundary data, we notice immediately that the EPSG code isn't accurate. The EPSG code of Thailand is 32647 while the data-frame has an EPSG code of 4326.

We use the st_transform() function to convert the EPSG code to the correct value of 32647.

```{r}
map_sf=st_transform(map_sf,crs = 32647)
map2_sf=st_transform(map2_sf,crs = 32647)
```

Given that the OSM data-frame has no CRS information, we will first initialize it to WGS84 using the default EPSG code of 4326 by using the st_set_crs() function of the sf package.

```{r}
#| eval: false
road_sf=st_set_crs(road_sf, 4326)
```

We will then implement the st_crs() function to verify if the boundary data has the accurate CRS information after we transformed it above.

```{r}
st_crs(map_sf)


```

## 1.2.3 Data Preparation and Wrangling

Given our area of interest is the Bangkok Metropolitan Region, we create a list of all provinces in this area in order to facilitate filtering for our analysis.

```{r}
bmr_province=c("Bangkok", "Samut Prakan", "Samut Sakhon", "Nonthaburi", "Nakhon Pathom", "Pathum Thani")
```

Below, we implement the filter() function in order to filter the boundary data, at the province and district level, as well as the accident data, down to our region of interest- the Bangkok Metropolitan Region.

```{r}
bmr_boundary=map_sf %>% 
  filter(ADM1_EN %in% bmr_province)
bmr_boundary2=map2_sf %>% 
  filter(ADM1_EN %in% bmr_province)
bmr_accsf=rdacc_sf %>% 
  filter(province_en %in% bmr_province)

```

```{r}
st_bbox(bmr_boundary)
```

We now implement the st_crs() function in order to transform the EPSG code of the `road_sf` data-frame from 4326 to 32647. This allows us to have the accurate geometry values to facilitate future spatial joins, as well as analysis.

```{r}
#| eval: false
# If road_sf was originally in EPSG:4326, reproject it properly
road_sf <- st_transform(road_sf, crs = 32647)

```

After ensuring that the CRS values are consistent, we can proceed with implementing the st_intersection() function and join the two data-frames based on their geometries.

The st_intersection() function is applied at the province level below.

```{r}
#| eval: false
bmr_roads=st_intersection(map2_sf, road_sf)

```

As earlier, we will save this as an RDS file in order to improve computational efficiency.

```{r}
#| eval: false
write_rds(bmr_roads, "data/rds/bmrroads")
```

```{r}
bmr_roads=read_rds("data/rds/bmrroads")
```

Below, the st_intersection() function is applied at the district level and saved as an RDS file.

```{r}
#| eval: false
bmr_districts=st_intersection(map2_sf, bmr_roads)
write_rds("data/rds/bmr_districts_roads")
```

```{r}
bmr_districts=read_rds("data/rds/bmr_districts_roads")
```

# 1.3 Exploratory Data Analysis

To start of our analysis, we will first create a simple bar plot using the ggplot2 package to visualize the number of accidents per province.

```{r}
# Step 1: Summarize the number of accidents per province
accident_summary <- bmr_accsf %>%
  group_by(province_en) %>%
  summarize(total_accidents = n())

# Step 2: Visualize using ggplot2
ggplot(accident_summary, aes(x = reorder(province_en, total_accidents), y = total_accidents)) +
  geom_bar(stat = "identity", fill = "darkred") +
  coord_flip() +  # Flip coordinates for better readability if many provinces
  labs(title = "Total Accidents by Province",
       x = "Province",
       y = "Number of Accidents") +
  theme_minimal()


```

Bangkok appears to have the most accidents followed by Samut Prakan.

Below, we use the tmap package to plot a highly cartographic map to verify the above information. We also use a basemap, thanks to OpenStreetMap, to be able to understand further where accidents occur.

This interactive map allows us to click on a point and identify the reason for the accident, the time, the month amongst other factors.

```{r}
# Set tmap to interactive view mode (optional for exploration)
tmap_mode("plot")

# Create the map with boundaries and accident points
tm_shape(bmr_boundary2) +
  tm_borders(col = "black") +  # Show the boundaries
  tm_shape(bmr_accsf) +
  tm_dots(col = "red", size = 0.01) +  # Show accident points as red dots
  tm_layout(frame = FALSE, legend.outside = TRUE)

# Switch back to static mode after rendering (if needed)
tmap_mode("plot")
```

From the map above, we can infer the Eastern and South-Eastern region seems to have a high concentration of road accidents. We will verify if this is true using Spatial Analysis techniques later on in this exercise.

There appears to be a significant amount of rollover/driving off the curve accidents on curvy roads, especially when it is rainy.

We further break it down and produce a hexagonal choropleth map to gain a more in-depth understanding of the distribution of accidents across the region.

Below, we start by initializing a variable `grid_size` (2000 meters), and then use the st_make_grid() function of the sf package to create a grid that covers our region of interest. After creating the grid, we will convert to an sf data-frame using the st_sf() function and then ensure that we only have hexagons within the region of interest by implementing the st_intersection() function.

```{r}
grid_size <- 2000  # Size of the grid cells in meters (adjust as necessary)

# Create a hexagonal grid covering the BMR boundary
bmr_hex_grid <- st_make_grid(bmr_boundary, cellsize = grid_size, square = FALSE)

# Convert to an sf object and clip to the BMR boundary
bmr_hex_grid_sf <- st_sf(geometry = bmr_hex_grid)
bmr_hex_grid_sf <- st_intersection(bmr_hex_grid_sf, bmr_boundary)
```

We can create the map using functions of the tmap() package.

```{r}
# Count accidents in the hexagonal grid
accidents_in_hex_grid <- st_join(bmr_hex_grid_sf, bmr_accsf) %>%
  group_by(geometry) %>%
  summarise(accident_count = n())

# Create a polygon heatmap using tmap for the hexagonal grid
tm_shape(accidents_in_hex_grid) +
  tm_polygons("accident_count", 
              palette = "Blues", 
              title = "Accident Count (Hexagonal Grid)") +
  tm_layout(title = "Accident Heatmap (Hexagonal Grid)", 
            legend.outside = TRUE)
```

We see above that the Eastern, especially towards Samut Prakan, region of the Bangkok Metropolitan region sees more accidents than the rest of the region. These hotspots are areas of interest and roads and speeding laws in these area must be looked at in greater detail and improved.

We will now implement the ggplot2 package to compare the number of accidents in the peak period vs the off-peak period in Bagnkok.

```{r}
# Count the number of accidents in Peak and Off-Peak periods
accident_counts_summary <- bmr_accsf %>%
  group_by(traffic_period) %>%
  summarise(accident_count = n())
# Create a bar chart showing the number of accidents in Peak vs Off-Peak
ggplot(data = accident_counts_summary, aes(x = traffic_period, y = accident_count)) +
  geom_bar(stat = "identity", fill = "steelblue") +  # Bar chart with specified accident counts
  labs(title = "Number of Accidents: Peak vs Off-Peak", 
       x = "Traffic Period", 
       y = "Number of Accidents") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))  # Center the title


```

Off-peak hours (Midnight to 7AM, 9AM to 4PM, 7PM to Midnight) sees more accidents in total.

We now move on and create a data-frame containing the count of the causes of each accident in the region.

```{r}
cause_count <- bmr_accsf %>%
  group_by(presumed_cause) %>%
  summarise(count = n()) 
```

We can implement functions of the ggplot2 package to create a bar plot.

```{r}
# Filter the top 5 causes
top_5_causes <- cause_count %>%
  top_n(5, count)

# Create the bar chart showing only the top 5 presumed causes
ggplot(top_5_causes, aes(x = reorder(presumed_cause, -count), y = count)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  geom_text(aes(label = count), vjust = -0.5) +  # Add labels to show the count on top of the bars
  theme_minimal() +
  labs(title = "Top 5 Presumed Causes of Accidents", 
       x = "Presumed Cause", 
       y = "Accident Count") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
```

Speeding appears to be the primary cause of accidents, responsible for over 80% of all road accidents in the Bangkok Metropolitan Region.

We now convert the sf data-frames into spatial objects to store in our environment to facilitate future spatial analysis. We implement the as_Spatial() function of the sf package for this. We will then write large files as rds files in the interest of computational efficiency.

```{r}

bmr_acc <- as_Spatial(bmr_accsf)
bmr <- as_Spatial(bmr_boundary)

```

```{r}
#| eval: false
write_rds(osm_data, "data/rds/osm_data_spatial")

```

```{r}
osm_data=read_rds("data/rds/osm_data_spatial")
```

We now convert the above obtain spatial object into Spatial Point and Spatial Polygon form respectively based on their geometry.

```{r}

bmracc_sp <- as(bmr_acc, "SpatialPoints")
boundary_sp <- as(bmr, "SpatialPolygons")
```

Now, we want to determine the road type that has the most accidents.

For this, we create a buffer first as accidents don't always happen directly on the road but just off it using the st_buffer() function. A buffer of 5 meters is created.

After that, we can perform a spatial join using the st_join() function of the sf package.

```{r}
# Create a small buffer around the accident points (e.g., 50 meters)
accidents_buffered = st_buffer(bmr_accsf, dist = 10)

# Perform the spatial join again with the buffered accident points
accidents_on_roads = st_join(accidents_buffered, bmr_roads, join = st_intersects)
head(accidents_on_roads)


```

We now count the number of accidents on each highway type by implementing the code chunk below.

```{r}
# Count accidents by road type
accidents_by_road_type = accidents_on_roads %>%
  group_by(highway) %>%  # Assuming the OSM roads data has a 'road_type' column
  summarise(accident_count = n())

# View the summary
print(accidents_by_road_type)
top_5_highway=accidents_by_road_type %>%
  arrange(desc(accident_count))%>%
  slice(1:5)
```

We will now use the ggplot2 package to produce a barplot to facilitate visualization and understand what type of highways result in the most accidents.

```{r}
library(ggplot2)

# Create bar plot
ggplot(top_5_highway, aes(x = highway, y = accident_count, fill = highway)) +
  geom_bar(stat = "identity") +
  labs(title = "Accident Count by highway Type", x = "Highway Type", y = "Number of Accidents") +
  theme_minimal()



```

We will now single out the motorway to further determine what type of accidents are most common on them.

```{r}
# Filter for motorway and count causes of accidents
motorway_accidents_cause_count <- accidents_on_roads %>%
  filter(highway == "motorway") %>%
  group_by(presumed_cause) %>%
  summarise(count = n()) %>%
  arrange(desc(count)) %>%   # Sort by count in descending order
  slice(1:5)                 # Select top 5 causes

# Create a bar plot for the top 5 causes of accidents on motorways
ggplot(motorway_accidents_cause_count, aes(x = reorder(presumed_cause, count), y = count, fill = presumed_cause)) +
  geom_bar(stat = "identity") +
  coord_flip() +  # Flip the axes for better readability
  labs(title = "Top 5 Causes of Accidents on Motorways",
       x = "Accident Cause",
       y = "Count") +
  theme_minimal() +
  theme(legend.position = "none")  # Hide legend


```

Speeding seems to be by far the most common reason for road accidents across the region.

The best way to deal with this would be the installation of more speed cameras and harsher penalties, in the form of larger fines, for failing to adhere to regulations.

Currently, fines for speeding can be up to 4000B. Increasing this fine would be prudent and help reduce the amount of speeding accidents on motorways and across the region in general.

Now we analyze the accident types based on weather and time of day see if there are any changes based on changes in either factor. We will use the ggplot2 package for this.

::: panel-tabset
## Time

```{r}
# Step 1: Create time intervals for hour of day
accidents <- bmr_accsf %>%
  mutate(time_interval = case_when(
    hourofday >= 0 & hourofday < 6  ~ "0-6",
    hourofday >= 6 & hourofday < 12 ~ "6-12",
    hourofday >= 12 & hourofday < 18 ~ "12-18",
    hourofday >= 18 & hourofday <= 24 ~ "18-24"
  ))

# Group and count the causes of accidents by time interval
top_5_time <- accidents %>%
  group_by(time_interval, presumed_cause) %>%
  summarise(count = n()) %>%
  arrange(desc(count)) %>%
  group_by(time_interval) %>%
  top_n(5, wt = count)  # Get top 5 causes for each time interval

# Plot for time intervals
ggplot(top_5_time, aes(x = reorder(presumed_cause, count), y = count, fill = presumed_cause)) +
  geom_bar(stat = "identity", position = "dodge") +
  coord_flip() +  # Flip the coordinates for better readability
  facet_wrap(~ time_interval, scales = "free_y") +
  labs(title = "Top 5 Causes of Accidents by Time Interval",
       x = "Cause of Accident", y = "Count") +
  theme_minimal() +
  theme(legend.position = "none")

```

There doesn't seem to be a significant difference based on the different time periods of the day.

## Weather

```{r}
# Group and count the causes of accidents by weather
top_5_weather <- bmr_accsf %>%
  group_by(weather_condition, presumed_cause) %>%
  summarise(count = n()) %>%
  arrange(desc(count)) %>%
  ungroup() %>%
  top_n(5, wt = count)  # Get the top 5 causes overall (not grouped by weather)

# Plot for all weather conditions
ggplot(top_5_weather, aes(x = weather_condition, y = count, fill = presumed_cause)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Top 5 Causes of Accidents Across All Weather Conditions",
       x = "Weather Condition", y = "Count") +
  theme_minimal() +
  theme(legend.position = "bottom")  # Place legend at the bottom
```

Speeding, unsurprisingly, is the primary reason for road accidents. However, we see far more instances of accidents in clear conditions.
:::

# 1.4 Spatial Analysis

## 1.4.1 Kernel Density Estimation (KDE)

KDE will allow us to better estimate the distribution of accidents acrosss the Bangkok Metropolitan Region. Using this, we can make more informed decisions on where to focus resources on to reduce the rate of accidents across the region.

We start off by preparing the data for KDE.

```{r}

# Step 3: Define the window (bounding box) from the boundary
boundary_bbox <- bbox(boundary_sp)  # Get bounding box of boundary
window <- owin(xrange = boundary_bbox[1, ], yrange = boundary_bbox[2, ])  # Define window

# Step 4: Extract the coordinates from the SpatialPoints object
coordinates <- coordinates(bmracc_sp)

# Step 5: Convert the SpatialPoints into a ppp object using the window
bmracc_ppp <- ppp(x = coordinates[,1], y = coordinates[,2], window = window)

# Step 6: Check the structure of the resulting point pattern object
summary(bmracc_ppp)

# You can now use the ppp object for spatial analysis with spatstat
```

::: note-box
Note that you must ensure to convert the spatial objects into ppp (planar point pattern) form before proceeding with Kernel Density Estimation. The spatstat package requires data to be in this format before conducting point pattern analysis. This also improves computational efficiency.
:::

### 1.4.1.1 Dealing with Duplicates

After creating the above PPP object, we check for duplicates by implementing the code chunk below.

```{r}
any(duplicated(bmracc_ppp))
```

We check the number of duplicates in our data-frame using the code chunk below. The multiplicity() function creates a list of all duplicates after which the sum() function counts the total.

```{r}
sum(multiplicity(bmracc_ppp)>1)
```

There are over 2293 duplicates. In this case, it means we have 2293 ***overlapping*** data points.

Using the rjitter() function, we deal with these data points by spacing them out slightly around the point. The function adds noise to the data and spreads these points out a little to facilitate visualization.

```{r}
bmr_ppp_jit <- rjitter(bmracc_ppp, 
                             retry=TRUE, 
                             nsim=1, 
                             drop=TRUE)
```

### 1.4.1.2 Creating an owin object

When analyzing Spatial Point patterns, it is a good practice to confine the boundaries of the analysis with a Geographical area, such as Singapore’s boundary. Using the **spatstat** package, we can create an **owin** object that is designed to represent such polygonal regions.

We use the as.owin() function as shown in the code chunk below.

```{r}
bmr_owin <- as.owin(bmr_boundary)
```

We use the plot() function to verify if the owin object has been correctly created.

```{r}
plot(bmr_owin)
```

### 1.4.1.3 Computing KDE using automatic bandwidth selection

We use the **density()** function of the **spatstat** package to compute the kernel density. These are the key configurations used in the computation:

-   **bw.diggle()** automatic bandwidth selection method.

-   The smoothing kernel used in this instance is ***gaussian***. Other smoothing methods include *“epanechnikov”, “quartic”*. or *“disc”.*

```{r}
kde_bmracc_bw <- density(bmr_ppp_jit,
                              sigma=bw.diggle,
                              edge=TRUE,
                            kernel="gaussian") 
```

::: insights-box
The other two methods aside from bw.diggle() are bw.scott() and bw.ppl(). While bw.diggle() focuses on minimizing error in spatial density estimation for point process data and is tailor-made for spatial applications, bw.scott() (Scott’s rule) provides a rule-of-thumb bandwidth and is used in several KDE applications across different types of data besides just spatial data. bw.ppl() uses a more complex and data-driven approach (plug-in) for selecting the bandwidth, aiming to minimize the error in KDE. Like bw.diggle(), It is also tailor-made for spatial point processes, however, it takes a slightly different approach to bw.diggle()
:::

We will now plot the above using the plot() function.

```{r}
plot(kde_bmracc_bw)
```

Immediately we notice that the density values are very small. This is because the unit of measurement is in meter, meaning that the density values are computed with a unit of ‘number of points per square meter.’

We now use the rescale.ppp() function of the spatstat package to convert the unit of measurement from meter to kilometer. This is done by implementing the code chunk below.

```{r}
bmracc_ppp.km <- rescale.ppp(bmr_ppp_jit, 1000, "km")

```

After this, we re-deploy the density() function using the re-scaled data and plot the KDE map.

```{r}
kde_bmracc.bw <- density(bmracc_ppp.km, 
                         sigma=bw.diggle, 
                         edge=TRUE, 
                         kernel="gaussian")
plot(kde_bmracc.bw)
```

### 1.4.1.4 Converting the KDE output into a grid object

#### 1.4.1.4.1 Gridded raster

We convert the gridded kernel density objects into RasterLayer object by using **raster()** of the **raster** package.

```{r}
kde_bmracc_bw_raster <- raster(kde_bmracc.bw)
```

The CRS above is NA, so we now set the CRS to EPSG 32647 of Thailand.

After that, we can implement the plot() function to plot the same.

```{r}
projection(kde_bmracc_bw_raster) <- CRS("+init=EPSG:32647")
plot(kde_bmracc_bw_raster)
```

From the above, we infer that the South-East region, specifically districts around the Samut Prakan province seem to have a higher density of accidents.

### 1.4.1.4.2 Peak vs Off-Peak Hours KDE \[Spatio-Temporal Analysis)

## 1.4.1.5 Network Kernel Density Estimation (NKDE)

By completing Kernel Density Estimation, we have identified two provinces of interest, Bangkok and Samut Prakan.

We will first extract our target regions using the code chunk below.

```{r}
smt_prk_boundary= bmr_boundary2%>%
  filter(ADM1_EN=='Samut Prakan')
smt_prk_roads= bmr_roads%>%
  filter(ADM1_EN=='Samut Prakan')
```

```{r}
# We now extract the three districts with the highest number of accidents in Bangkok
bkk_boundary= bmr_boundary2%>%
  filter(ADM2_EN %in% c("Lat Krabang", "Saphan Sung", "Khan Na Yao"))
bkk_roads= bmr_districts%>%
  filter(ADM2_EN %in% c("Lat Krabang", "Saphan Sung", "Khan Na Yao"))
```

We will now join the OpenStreetMap data, `smt_prk_roads` and `bkk_roads`, using st_intersection function of the sf package.

```{r}
#Samut Prakan
smt_pkn_roads_intersection=st_intersection(smt_prk_roads, smt_prk_boundary)
```

```{r}
# Bangkok Districts
bkk_roads_intersection=st_intersection(bkk_roads, bkk_boundary)
```

We will now filter down our accident data, `bmr_accsf`, down to our target regions.

```{r}
#Samut Prakan
smt_pkn_acc=bmr_accsf%>%
  filter(province_en=='Samut Prakan')
```

```{r}
#Bangkok Districts
bkk_acc=bmr_accsf%>%
  filter(province_en=='Bangkok')
district_accidents=st_intersection(bkk_acc, bkk_boundary)
```

We can now start with Network Kernel Density Estimation.

We must first cut the SpatialLines object into Lixels by implementing the lixelize_lines() function with a specified minimal distance.

```{r}
# We first ensure that the geometries are in the right format for lixelization.
smt_pkn_roads_intersection=st_cast(smt_pkn_roads_intersection, 'LINESTRING', group_or_split= TRUE)
samut_prakan_lines=st_cast(smt_pkn_roads_intersection,'LINESTRING')
lixels_smt_pkn <- lixelize_lines(smt_pkn_roads_intersection, 
                         10000, 
                         mindist = 5000)
```

```{r}
# Filter out POINT geometries
bkk_roads_lines <- bkk_roads_intersection[st_is(bkk_roads_intersection, c("LINESTRING", "MULTILINESTRING")), ]

# Cast MULTILINESTRING to LINESTRING
bkk_roads_linestring <- st_cast(bkk_roads_lines, "LINESTRING", group_or_split = TRUE)

# Create Lixels
lixels_bkk <- lixelize_lines(bkk_roads_linestring, 
                         10000, 
                         mindist = 5000)
```

::: insights-box
There is another function, **lixelize_lines.mc()** that provides multicore support and is typically used in spatial analysis or geospatial data processing, specifically in contexts where large datasets of lines (such as roads, paths, or boundaries) need to be broken down into smaller, equally spaced segments or “lixels” (line pixels). This process is known as “lixelization.”

The main purpose of the lixelize_lines.mc() function is to improve the efficiency of the lixelization process by utilizing multiple CPU cores simultaneously. This is particularly beneficial when dealing with large datasets, as processing each line sequentially can be time-consuming.
:::

We now proceed to generate a SpatialPointsDataFrame with line centre points using the lines_center() function of spNetwork.

```{r}
samples_smt_pkn <- lines_center(lixels_smt_pkn) 
```

```{r}
samples_bkk <- lines_center(lixels_bkk)
```

Now, we can perform NKDE. We use the nkde() function to carry it out.

```{r}
#| eval: false
# Samut Prakan NKDE
densities_smt_prk <- nkde(lixels_smt_pkn, 
                  events = smt_pkn_acc,
                  w = rep(1, nrow(smt_pkn_acc)),
                  samples = samples_smt_pkn,
                  kernel_name = "quartic",
                  bw = 1000, 
                  method = "simple")
```

```{r}
#| eval: false
write_rds(densities_smt_prk, "data/rds/density_samut")
```

```{r}
densities_smt_prk=read_rds('data/rds/density_samut')
```

```{r}
#| eval: false
# Perform NKDE for Bangkok Districts
densities_bkk <- nkde(
  lines = lixels_bkk    ,               # Road network
  events = district_accidents,      # Subset of accident data (for testing)
  w = rep(1,nrow(district_accidents)),                # Weights (equal for all events)
  samples = samples_bkk,           # Sample points along the road network
  kernel_name = "quartic",         # Kernel type (quartic kernel)
  bw = 500, # Bandwidth (smoothing parameter)
  div = 'bw',
  method = "simple",               # Simple method for NKDE,
  verbose = TRUE
  )          # Spatial grid resolution (lower for faster computation)                   
write_rds(densities_bkk, 'data/rds/density_bkk')
```

```{r}
densities_bkk=read_rds('data/rds/density_bkk')
```

::: insights-box
-   The `kernel_name` argument specifies the type of kernel function used.

-   Possible kernel methods supported by `spNetwork` include:

    -   Quartic

    -   Triangle

    -   Gaussian

    -   Scaled Gaussian

    -   Tricube

    -   Cosine

    -   Triweight

    -   Epanechnikov

    -   Uniform

-   The `method` argument indicates the method used to calculate Network Kernel Density Estimation (NKDE).

-   `spNetwork` supports three popular methods:

    1.  **Simple (`method = "simple"`)**:

        -   Introduced by Xie et al. (2008).

        -   Distances between events and sampling points are replaced by network distances.

        -   The kernel formula is adapted to calculate density over a linear unit instead of an areal unit.

    2.  **Discontinuous (`method = "discontinuous"`)**:

        -   Proposed by Okabe et al. (2008).

        -   Divides the mass density of an event at intersections of lixels.

        -   Results in a discontinuous kernel function.

    3.  **Continuous (`method = "continuous"`)**:

        -   Also proposed by Okabe et al. (2008).

        -   Adjusts the density before intersections to create a continuous kernel function.

        -   Still divides the mass of the density at intersections but with a continuous adjustment.
:::

We will now visualize NKDE.

We must first insert the computed density values into samples and lixels objects as the density field.

```{r}
#Samut Prakan
samples_smt_pkn$density<- densities_smt_prk
lixels_smt_pkn$density <- densities_smt_prk
```

```{r}
#Bangkok Districts
samples_bkk$density<- densities_bkk
lixels_bkk$density <- densities_bkk
```

We now upscale the density for both in the interest of easier understanding.

```{r}
#Samut Prakan
samples_smt_pkn$density <- samples_smt_pkn$density*1000
lixels_smt_pkn$density <- lixels_smt_pkn$density*1000
```

```{r}
#Bangkok Districts
samples_bkk$density <- samples_bkk$density*1000
lixels_bkk$density <- lixels_bkk$density*1000
```

We can now plot the map as the data is now prepared.

We will use the tmap package to produce a highly cartographic and interactive map.

```{r}
#Samut Prakan
tmap_mode('plot')
tm_shape(lixels_smt_pkn)+
  tm_lines(col="density")+
tm_shape(smt_pkn_acc)+
  tm_dots()
```

The interactive map above effectively reveals road segments with relatively higher density of accidents than road segments with relatively lower density of accidents with the help of shading. The roads with darker shades have a relatively higher density.

```{r}
#Bangkok Districts
tmap_mode('plot')
tm_shape(lixels_bkk)+
  tm_lines(col="density")
```

We see a highway on the North West side, across roads such as Kanchanapishek Road, of the three districts have a particularly high density of accidents on it after examining the plots above. This can be further investigated in order to develop policies and improve roads in those specific regions.

From all our analysis, we take away that the south-eastern part of Bangkok Metropolitan Region was a hotspot, and this map above allows us to further understand the distribution of accidents across the region.

### 1.4.1.5.1 Network Constrained G- and K-Function Analysis

We will now conduct a test for Complete Spatial Randomness by using the **kfunctions()** function of the **spNetwork** package.

The hypotheses are as follows:

-   Ho: The observed spatial point events (i.e distribution of accidents) are uniformly distributed over a street network in Samut Prakan.

-   H1: The observed spatial point events (i.e: distribution of accidents) are not uniformly distributed over a street network in Samut Prakan.

```{r}
#| eval: false
# Samut Prakan
kfun_accidents <- kfunctions(smt_pkn_roads_intersection, 
                             smt_pkn_acc,
                             start = 0, 
                             end = 1000, 
                             step = 50, 
                             width = 50, 
                             nsim = 50, 
                             resolution = 50,
                             verbose = FALSE, 
                             conf_int = 0.05,
                             agg = 1)  # Set aggregation to merge close points
```

```{r}
#| eval: false
write_rds(kfun_accidents, "data/rds/kfunction")
```

```{r}
kfun_accidents=read_rds('data/rds/kfunction')
```

We now do the same for the selected Bangkok Districts.

```{r}
#| eval: false
# Manually snap points with a larger tolerance
snapped_points <- st_snap(district_accidents, bkk_roads_linestring, tolerance = 100)

# Now run kfunctions on the snapped points
kfun_accidents_bkk <- kfunctions(
  bkk_roads_linestring, 
  snapped_points,
  start = 0, 
  end = 1000, 
  step = 50, 
  width = 50, 
  nsim = 50, 
  resolution = 50,
  verbose = FALSE, 
  conf_int = 0.05,
  agg = 500  # Reasonable aggregation value
)

write_rds(kfun_accidents_bkk, 'data/rds/kfunction_bkk')

```

```{r}
kfun_accidents_bkk=read_rds('data/rds/kfunction_bkk')
```

::: insights-box
Below are the arguments of the function above:

-   **lines**: A `SpatialLinesDataFrame` containing the sampling points. The geometries must be valid; using invalid geometries may cause the process to crash.

-   **points**: A `SpatialPointsDataFrame` representing the points on the network. These points will be snapped to the network for analysis.

-   **start**: A numeric value indicating the starting point for evaluating the k and g functions.

-   **end**: A numeric value specifying the endpoint for evaluating the k and g functions.

-   **step**: A numeric value that determines the interval between evaluations of the k and g functions.

-   **width**: The width of each “donut” or ring used in calculating the g-function.

-   **nsim**: An integer representing the number of Monte Carlo simulations to perform. In the example above, 50 simulations were conducted; however, more simulations are often necessary for accurate inference.

-   **resolution**: Specifies the resolution when simulating random points on the network. A higher resolution can significantly reduce calculation time. If set to `NULL`, random points can occur anywhere on the network. If a value is provided, the network’s edges are divided according to this resolution, and random points are selected from the vertices of the newly segmented network.

-   **conf_int**: A numeric value indicating the confidence interval width, with the default set to 0.05.
:::

The k-function will output the following:

-   *plotk,* a ggplot2 object representing the values of the k-function.

-   *plotg*, a ggplot2 object representing the values of the g-function.

-   *values*, a DataFrame with the values used to build the plots.

Below, we visualize the plots generated.

We start with **Samut Prakan**.

::: panel-tabset
## Plot K

```{r}
kfun_accidents$plotk
```

Based on the plot above, we can see the the blue line are entirely above the envelope at the bottom, indicating that we do indeed have sufficient evidence to reject the null hypothesis and conclude that the distribution of accidents across Samut Prakan do exhibit spatial clustering tendencies.

## Plot G

```{r}
kfun_accidents$plotg
```

Based on the plot above, we can see that the blue line is entirely above the envelope at the bottom, similar to the k plot, indicating that we do indeed have sufficient evidence to reject the null hypothesis and conclude that the distribution of accidents across Samut Prakan do exhibit Spatial Clustering tendencies.
:::

We will now plot the K and G plot for the selected **Bangkok Districts**.

::: panel-tabset
## K Plot

```{r}
plot(kfun_accidents_bkk$plotk)
```

## G Function

```{r}
plot(kfun_accidents_bkk$plotg)
```
:::

## 1.4.2 Second-Order Spatial Point Pattern Analysis.

We now focus on Second Order Spatial Point Patterns Analysis.

We first filter the data down to the regions of interest and create separate boundaries for each of them. Following this, we produce an owin object using the as.owin() function.

```{r}
# Filtering the data down to the regions of interest
bkk = bmr_boundary %>%
  filter(ADM1_EN == "Bangkok")
smt_pkn = bmr_boundary %>%
  filter(ADM1_EN == "Samut Prakan")
ntbr = bmr_boundary %>%
  filter(ADM1_EN == "Nonthaburi")
nkn_ptn= bmr_boundary %>%
  filter(ADM1_EN == "Nakhon Pathom")
smt_skn= bmr_boundary %>%
  filter(ADM1_EN == "Samut Sakhon")
ptm_thn= bmr_boundary%>%
  filter(ADM1_EN=='Pathum Thani')

# Creating the owin object
bkk_owin = as.owin(bkk)
smt_pkn_owin = as.owin(smt_pkn)
ntbr_owin = as.owin(ntbr)
nkn_ptn_owin = as.owin(nkn_ptn)
smt_skn_owin = as.owin(smt_skn)
ptm_thn_owin= as.owin(ptm_thn)

#Creating point planar patterns
acc_bkk_ppp = bmr_ppp_jit[bkk_owin]
acc_smt_pkn_ppp = bmr_ppp_jit[smt_pkn_owin]
acc_ntbr_ppp = bmr_ppp_jit[ntbr_owin]
acc_nkn_ptn_ppp = bmr_ppp_jit[nkn_ptn_owin]
acc_smt_skn_ppp= bmr_ppp_jit[smt_skn_owin]
acc_ptm_thn_ppp= bmr_ppp_jit[ptm_thn_owin]

```

### 1.4.2.1 Calculating G-function estimates and testing for Complete Spatial Randomness

We now focus on computing G-function estimates.

The test below allows us to understand if accidents in the Bangkok Metropolitan Region exhibit Spatial Clustering or if they are Randomly Distributed.

We apply the test for Complete Spatial Randomness to each region individually. The tests are conducted at a 5% significance level.

The hypotheses are as follows.

-   ***H0: There is no spatial clustering. The accidents are randomly distributed across the region.***

-   ***H1: There is spatial clustering. The accidents are NOT randomly distributed across the region.***

::: panel-tabset
## Bangkok

We use the Gest() function of the spatstat package to compute a G-function estimation for Bangkok. Following that, we plot the result.

```{r}
G_bkk = Gest(acc_bkk_ppp, correction = "border")
plot(G_bkk, xlim=c(0,500))
```

We now carry out the monte carlo simulation test, using the envelope() function, for complete spatial randomness.

```{r}
G_bkk.csr <- envelope(acc_bkk_ppp, Gest, nsim = 99)

```

```{r}
plot(G_bkk.csr)
```

It is clear that that there is clustering exhibited in this scenario as the black line, observed values, is far above the envelope. We have sufficient evidence to reject the null hypothesis.

## Samut Prakhan

We use the Gest() function of the spatstat package to compute a G-function estimation for Samut Prakhan. Following that, we plot the result.

```{r}
G_smt_pkn = Gest(acc_smt_pkn_ppp, correction = "border")
plot(G_smt_pkn, xlim=c(0,500))
```

We now implement a monte carlo simulation test, using the envelope() function, for Complete Spatial Random in Samut Prakan.

```{r}
G_smt_pkn.csr <- envelope(acc_smt_pkn_ppp, Gest, nsim = 199)
```

```{r}
plot(G_smt_pkn.csr)
```

We once again have sufficient evidence to reject the null hypothesis and conclude that accidents in Samut Prakan do indeed exhibit clustering, more so than is expected in a region that is randomly distributed.

## Nonthaburi

We use the Gest() function of the spatstat package to compute a G-function estimation for Nonthaburi. Following that, we plot the result.

```{r}
G_ntbr = Gest(acc_ntbr_ppp, correction = "border")
plot(G_ntbr, xlim=c(0,500))
```

We now implement a Monte-Carlo Simulation test, using the envelope() function, to check for complete spatial randomness of roads accidents in Nonthaburi.

```{r}
G_ntbr.csr <- envelope(acc_ntbr_ppp, Gest, nsim = 199)
```

```{r}
plot(G_ntbr.csr)
```

We once again have sufficient evidence to reject the null hypothesis and conclude that accidents in Nonthaburi do indeed exhibit clustering, more than is expected in a region that is randomly distributed.

## Nakhon Pathom

We use the Gest() function of the spatstat package to compute a G-function estimation for Nakhon Pathom. Following that, we plot the result.

```{r}
G_nkn_ptn = Gest(acc_nkn_ptn_ppp, correction = "border")
plot(G_nkn_ptn, xlim=c(0,500))
```

We now conduct the Monte-Carlo Simulation test, using the envelope() function, for complete spatial randomness for accidents in Nakhon Pathom.

```{r}
G_nkn_ptn.csr <- envelope(acc_nkn_ptn_ppp, Gest, nsim = 199)
```

```{r}
plot(G_nkn_ptn.csr)
```

We once again have sufficient evidence to reject the null hypothesis and conclude that accidents in Nakhon Pathom do indeed exhibit clustering, more than is expected in a region that is randomly distributed.

## Samut Sakhon

We use the Gest() function of the spatstat package to compute a G-function estimation for Samut Sakhon. Following that, we plot the result.

```{r}
G_smt_skn = Gest(acc_smt_skn_ppp, correction = "border")
plot(G_smt_skn, xlim=c(0,500))
```

We now conduct the Monte-Carlo Simulation test, using the envelope() function, for complete spatial randomness for accidents in Samut Sakhon.

```{r}
G_smt_skn.csr <- envelope(acc_smt_skn_ppp, Gest, nsim = 199)
```

```{r}
plot(G_smt_skn.csr)
```

We once again have sufficient evidence to reject the null hypothesis and conclude that accidents in Samut Sakhon do indeed exhibit clustering, more so than is expected in a region that is randomly distributed.

## Patum Thani

We use the Gest() function of the Spatstat package to compute a G-function estimation for Patum Thani

```{r}
G_ptm_thn = Gest(acc_ptm_thn_ppp, correction = "border")
plot(G_nkn_ptn, xlim=c(0,500))
```

We now conduct the Monte Carlo Simulation Test for Complete Spatial Randomness by implementing the envelope() function.

```{r}
G_ptm_thn.csr <- envelope(acc_ptm_thn_ppp, Gest, nsim = 199)
```

```{r}
plot(G_ptm_thn.csr)
```

We have sufficient Evidence to reject the null hypothesis and can conclude that accidents in Pathum Thani are not randomly distributed and do indeed exhibit spatial clustering.
:::

::: insights-box
-   The grey zone indicates the confidence envelop (In this case, we have set it as 95% as indicated by the critical value of 0.05)

-   When an observed L value is greater than its corresponding L(theo) value for a particular distance and lower than the upper confidence envelop, spatial clustering for that distance is statistically NOT significant (e.g. distance between B and C).

-   When an observed L value is smaller than its corresponding L(theo) value for a particular distance and beyond the lower confidence envelop, spatial dispersion for that distance is statistically significant. - When an observed L value is smaller than its corresponding L(theo) value for a particular distance and within the lower confidence envelop, spatial dispersion for that distance is statistically NOT significant (e.g. distance between A and B).
:::

#### 1.4.2.2 Clark and Evans Test

We now perform the Clark-Evans test of Aggregation for a spatial point pattern by using the clarkevans.test() function of spatstat.

The Clark and Evans is a method used to analyze the spatial distribution of points in selected areas of interest. It allows us to understand whether a point pattern is Random, Clustered or Regularly Spaced by comparing the observed nearest-neighbor distances between points to the expected distances under a random distribution.

The hypotheses that we will be testing are as follows:

Ho = The distribution of accidents are randomly distributed.

H1= The distribution of accidents are not randomly distributed.

The tests will be conducted at a 5% significance level.

::: panel-tabset
## Bangkok

```{r}
clarkevans.test(acc_bkk_ppp,
                correction="none",
                clipregion=NULL,
                alternative=c("two.sided"),
                nsim=999)

```

## Samut Prakhan

```{r}
clarkevans.test(acc_smt_pkn_ppp,
                correction="none",
                clipregion=NULL,
                alternative=c("two.sided"),
                nsim=999)
```

## Samut Sakhon

```{r}
clarkevans.test(acc_smt_skn_ppp,
                correction="none",
                clipregion=NULL,
                alternative=c("two.sided"),
                nsim=999)
```

## Nonthaburi

```{r}
clarkevans.test(acc_ntbr_ppp,
                correction="none",
                clipregion=NULL,
                alternative=c("two.sided"),
                nsim=999)
```

## Nakhon Pathom

```{r}
clarkevans.test(acc_nkn_ptn_ppp,
                correction="none",
                clipregion=NULL,
                alternative=c("two.sided"),
                nsim=999)
```

## Patum Thani

```{r}
clarkevans.test(acc_ptm_thn_ppp,
                correction="none",
                clipregion=NULL,
                alternative=c("two.sided"),
                nsim=999)
```
:::

From the above outputs, we are able to reject the null hypothesis given the observed p-value is less than 0.05. We have sufficient evidence and can conclude that the distribution of accidents across the Bangkok Metropolitan Region are not randomly distributed and do indeed exhibit spatial clustering.

::: insights-box
The R value is an important part of the output.

-   When R=1, it means there is random spatial distribution.

-   When R\>1 It means that there is Regularly Spaced Spatial Distribution

-   When R\<1, it means that there is Clustered Spatial Distribution.
:::

## 1.4.3 Spatial Weights

We now create a data-frame, `boundary_with_accident_count`, in order to assign Spatial Weights and carry out further analysis.

We start off by conducting a spatial join with the help of the st_join() function of the sf package to join `bmr_accsf`, accident data of the Bangkok Metropolitan Region.

We then use the left_join() function of the dplyr package to join this with the boundary data in order to facilitate analysis.

We then save it as an RDS file.

```{r}
#| eval: false
#| # Perform a spatial join to assign accidents to districts
accidents_with_districts <- st_join(bmr_accsf, bmr_boundary2, join = st_intersects)
# Group by adm2_en and count the number of accidents per district
accident_counts_by_adm2 <- accidents_with_districts %>%
  group_by(ADM2_EN) %>%
  summarise(accident_count = n())

# Perform a left join with the boundary data
boundary_with_accident_count <- bmr_boundary2 %>%
  st_join(accident_counts_by_adm2, by = "ADM2_EN")
boundary_with_accident_count <- boundary_with_accident_count %>%
  mutate(accident_count = ifelse(is.na(accident_count), 0, accident_count))
write_rds(boundary_with_accident_count, "data/rds/boundary_with_acc_count")

```

```{r}
boundary_with_accident_count=read_rds("data/rds/boundary_with_acc_count")
```

After successfully completing the relational join, we can now plot a choropleth map to visualize the number of accidents in the Bangkok Metropolitan Region using various functions of the tmap package

```{r}
basemap <- tm_shape(boundary_with_accident_count) +
  tm_polygons() +
  tm_text("ADM2_EN.x", size=0.5)
gdppc <- qtm(boundary_with_accident_count, "accident_count")
tmap_arrange(basemap, gdppc, asp=1, ncol=2)

```

The above map reinforces our earlier finding that the Eastern/South-Eastern region of the Bangkok Metropolitan Region have the most accidents. One district in the bottom left, part of the Samut Sakhon province, also has a high volume of accidents.

### 1.4.3.1 Computing Contiguity Spatial Weights

We now implement the **poly2nb()** function of the **spdep** package to compute contiguity weight matrices for the study area selected.

Using this function, we are able to build a ‘neighbors list’ based on regions with contiguous boundaries.

In this function, we will pass an argument, ‘queen’, that can be set as either TRUE (default) or FALSE. If the ‘queen’ argument is not explicitly set to FALSE, the function returns a list of first order neighbors using the Queen criteria.

[You may refer to the `spdep` package documentation here](https://cran.r-project.org/web/packages/spdep/spdep.pdf) to learn more about its functions and arguments.

#### 1.4.3.1.1 Computing (QUEEN) contiguity based neighbors

The poly2nb() function is implemented as shown in the code chunk below. Using this, we are able to compute a Queen contiguity weight matrix.

```{r}
wm_q <- poly2nb(boundary_with_accident_count, queen=TRUE)
summary(wm_q)
```

The most connected area has 9 neighbors. In general, most districts have approximately 5 to 6 neighbors.

#### 1.4.3.1.2 Computing (ROOK) contiguity based neighbors

For this, we will set the `queen` argument of the poly2nb() function to false.

```{r}
wm_r <- poly2nb(boundary_with_accident_count, queen=FALSE) 
summary(wm_r)


```

The most connected area has 8 neighbors. In general, most districts have approximately 5 to 6 neighbors.

### 1.4.3.2 Visualizing contiguity weights

A connectivity graph takes a point and displays a line to each neighboring point. We are working with polygons in this situation, so we need to ensure that our points are in order to produce our connectivity graphs.

Usually, the method of choice will be polygon centroids. We calculate using the sf package before moving onto the graphs. Getting latitude and longitude of the Polygon Centroids.

We need points to associate with each polygon before we can make our connectivity graph. It won’t be as simple as applying the st_centroid() function of the sf sf object: *`us.bound`*. We need the coordinates in a separate data-frame for this to work.

To do this, we will use a mapping function which will apply a given function to each element of a vector and returns a vector of the same length. Our input vector will be the geometry column of `us.bound`.

The function that we implement in this situation will be st_centroid().

We will be using the map_dbl variation of map from the purrr package.

#### 1.4.3.2.1 Obtaining Coordinate values

::: panel-tabset
## Longitude

```{r}
longitude <- map_dbl(boundary_with_accident_count$geometry, ~st_centroid(.x)[[1]])
```

## Latitude

```{r}
latitude <- map_dbl(boundary_with_accident_count$geometry, ~st_centroid(.x)[[2]])
```

## Coords

Now that we have the latitude and longitude values, we can use the cbind() function to put the longitude and latitude values into the same object, `coords`.

```{r}
coords <- cbind(longitude, latitude)
```

```{r}
head(coords)
```
:::

We can now plot the contiguity weights side by side for comparison.

```{r}
par(mfrow=c(1,2))
plot(boundary_with_accident_count$geometry, border="lightgrey", main="Queen Contiguity")
plot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= "purple")
plot(boundary_with_accident_count$geometry, border="lightgrey", main="Rook Contiguity")
plot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = "darkgreen")


```

::: insights-box
-   Queen Contiguity creates more connections as seen above, as it considers both shared vertices AND edges. This results in a more dense network.

-   Rook Contiguity results in fewer connections as it only considers shared borders as opposed to Queen Contiguity. This results in a simpler network.
:::

We often use these in order to proceed with Spatial Autocorrelation Analysis to understand the neighbors taken into consideration amongst other factors.

### 1.4.3.3 Adaptive Distance Weight Matrix

One of the characteristics of fixed distance weight matrices is that the more densely settled areas (usually urban areas) tend to have more neighbors and the less densely settled areas (usually rural areas) tend to have lesser neighbors.

Having many neighbors smoothens the neighbor relationship across more neighbors.

It is possible to control the numbers of neighbors directly using k-nearest neighbors by either accepting asymmetric neighbors or imposing symmetry

```{r}
knn6 <- knn2nb(knearneigh(coords, k=6))
knn6
```

We immediately see a difference when comparing the above output to `wm_q` and `wm_r`. The number of non-zero links is higher.

We can create the plot for the same using the plot() function.

```{r}
plot(boundary_with_accident_count$geometry, border="lightgrey")
plot(knn6, coords, pch = 19, cex = 0.6, add = TRUE, col = "red")
```

### 1.4.3.4 Row standardized weights matrix

#### 1.4.3.4.1 Weights Based on Inversed Distance Weighting (IDW)

We first compute the distances between areas by implementing the **nbdists()** function of the **spdep** package.

```{r}
dist <- nbdists(wm_q, coords, longlat = FALSE)
ids <- lapply(dist, function(x) 1/(x))
ids
```

We now need to assign weights to each neighboring polygon. We use equal weights (style=“W”), where each neighboring polygon is assigned a weight of 1 divided by the number of neighbors.

This means each neighboring county’s weight is calculated as 1/(# of neighbors), and these weights are then used to sum the weighted income values.

While this method is intuitive for summarizing neighbors’ values, it has a drawback: polygons at the edges of the study area may rely on fewer neighbors, potentially skewing the spatial autocorrelation results.

<details>

<summary>Note on `style` argument</summary>

::: note-box
Note: For simplicity, we’ll use the style=“W” option in this example, but be aware that more robust options, such as style=“B”, are available.
:::

</details>

```{r}
rswm_q <- nb2listw(wm_q, style="W", zero.policy = TRUE)
rswm_q
```

Setting the argument `zero.policy` to TRUE allows for lists of non-neighbors. This should be used with caution as users may not be aware of missing neighbors in their data however setting `zero,policy` to FALSE would return an error.

The code chunk below is implemented to check the weights of the first polygons eight neighbors type:

```{r}
rswm_q$weights[10]
```

Using the same method, we derive a row standardized distance weight matrix by using the code chunk below.

```{r}
rswm_ids <- nb2listw(wm_q, glist=ids, style="B", zero.policy=TRUE)
rswm_ids
```

We now compute the average neighbor Accident count value for each polygon. We often refer to these values as Spatially Lagged Values.

```{r}
acc.lag <- lag.listw(rswm_q, boundary_with_accident_count$accident_count)
acc.lag
```

We can retrieve the Accident count for each by using the code Chunk Below.

```{r}
nb1 <- wm_q[[1]] #Shows the accident counts for the neighboring districts of region 1
nb1 <- boundary_with_accident_count$accident_count[nb1]
nb1
```

::: insights-box
A spatial lag with row-standardized weights means that each observation’s value is influenced by the average values of its neighboring observations. Specifically, the weights are standardized so that the sum of the weights for each observation equals one.

This approach ensures that the spatial lag is essentially the weighted average of the neighboring values.
:::

We can append the spatially lagged Accident values onto the `boundary_with_accident_count` sf data-frame by using the code chunk shown below.

```{r}
lag.list <- list(boundary_with_accident_count$ADM2_EN.x, lag.listw(rswm_q, boundary_with_accident_count$accident_count))
lag.res <- as.data.frame(lag.list)
colnames(lag.res) <- c("ADM2_EN.x", "lag Accident Count")
boundary_with_accident_count <- left_join(boundary_with_accident_count,lag.res)

```

We can gain further insight into this data-frame by implementing the head() function.

```{r}
head(boundary_with_accident_count)
```

We now plot the observed Accident and Spatial Lag Accidents side by side for comparison.

```{r}
acc_qtm <- qtm(boundary_with_accident_count, "accident_count")
lag_acc <- qtm(boundary_with_accident_count, "lag Accident Count")
tmap_arrange(acc_qtm, lag_acc, asp=1, ncol=2)



```

::: insights-box
The use of spatial weights are highly useful for a variety of reasons.

1.  They allow us to better understand and analyze the dependence between neighboring regions.

2.  The help us understand how accidents are not isolated incidents and are often a part of bigger spatial patterns. This allows us to make better decisions with regards to policy developments, resource allocation and more in order to reduce the number of accidents.

3.  These help us identify clusters and are crucial in identifying 'spillover' effects.
:::

### 1.4.3.5 Spatial Window Average

The spatial window average uses row-standardized weights and includes the diagonal element. To do this in R, we need to go back to the neighbors structure and add the diagonal element before assigning weights.

To add the diagonal element to the neighbour list, we use the include.self() function from the **spdep** package.

```{r}
wm_qs <- include.self(wm_q)
wm_qs
```

There is a difference in the key statistics shown above when compared to wm_q. The average number of links, the number of non-zero links as well as percentage of non-zero weights are all higher for wm_qs.

We look at the neighbor list of area \[1\] using the code chunk below.

```{r}
wm_qs[[1]]
```

This region has 9 neighbors.

We now implement the nb2listw() function in order to obtain weights.

```{r}
wm_qs <- nb2listw(wm_qs)
wm_qs
```

We now create the lag variable from our weight structure and `accident_count` variable.

```{r}
lag_w_avg_acc <- lag.listw(wm_qs, 
                             boundary_with_accident_count$accident_count)
lag_w_avg_acc
```

We then proceed to convert the lag variable `listw`object into a data-frame by using as.data.frame().

```{r}
lag.list.wm_qs <- list(boundary_with_accident_count$ADM2_EN.x, lag.listw(wm_qs, boundary_with_accident_count$accident_count))
lag_wm_qs.res <- as.data.frame(lag.list.wm_qs)
colnames(lag_wm_qs.res) <- c("ADM2_EN.x", "lag_window_avg acc")
```

::: note-box
Note: The third command line on the code chunk above renames the field names of *lag_wm_q1.res* object into accident_count and *lag_window_avg acc* respectively.
:::

We now append the lag_window_avg acc values onto the `boundary_with_accident_count` sf data.frame by using left_join() of **dplyr** package.

```{r}
boundary_with_accident_count <- left_join(boundary_with_accident_count, lag_wm_qs.res)
```

To compare the values of lag accident count and Spatial window average, The kable() function of the Knitr package is used to prepare a table.

```{r}
boundary_with_accident_count %>%
  dplyr::select("ADM2_EN.x", 
         "lag Accident Count", 
         "lag_window_avg acc") %>%
  kable()
```

We now plot the lag accident count and w_ave_acc maps next to each other for comparison using the qtm() function of the tmap package.

```{r}
w_avg_acc <- qtm(boundary_with_accident_count, "lag_window_avg acc")
tmap_arrange(lag_acc, w_avg_acc, asp=1, ncol=2)
```

```{r}
equal <- tm_shape(boundary_with_accident_count) +
  tm_fill("accident_count",
          n = 5,
          style = "equal") +
  tm_borders(alpha = 0.5) +
  tm_layout(main.title = "Equal interval classification")

quantile <- tm_shape(boundary_with_accident_count) +
  tm_fill("accident_count",
          n = 5,
          style = "quantile") +
  tm_borders(alpha = 0.5) +
  tm_layout(main.title = "Equal quantile classification")

tmap_arrange(equal, 
             quantile, 
             asp=1, 
             ncol=2)
```

## 1.4.4 Spatial Autocorrelation: Morans I test.

The hypotheses for the test are as follows:

-   ***H0: Regions with similar number of accidents are randomly distributed.***

-   ***H1: Regions with similar number of accidents are not randomly distributed and exhibit spatial clustering.***

```{r}
moran.test(boundary_with_accident_count$accident_count, 
           listw=rswm_q, 
           zero.policy = TRUE, 
           na.action=na.omit)
```

From the output above, we can infer the following:

-   The p-value (7.34e-05)\<0.05, indicating that the observed spatial autocorrelation is statistically significant.

-   Moran’s I statistic: The observed value of 0.236 indicates **positive spatial autocorrelation**, meaning that regions with similar number of accidents are more likely to be located near each other.

Since Moran’s I Statistic is significantly greater than what we would expect in a randomly distributed region. There is significant evidence to reject H0 and conclude that there is indeed spatial clustering with regards to Accidents in the Bangkok Metropolitan Region.

### 1.4.4.1 Monte Carlo Moran's I

We now implement the moran.mc() function of the spdep package. In this scenario, we will run 1000 simulations.

```{r}
bperm= moran.mc(boundary_with_accident_count$accident_count, 
                listw=rswm_q, 
                nsim=999, 
                zero.policy = TRUE, 
                na.action=na.omit)
bperm
```

Based on the above output, p-value (0.001)\<0.05, thus we can reject the null hypothesis at a 5% significance level and conclude that there is indeed spatial clustering.

### 1.4.4.2 Visualizing Monte Carlo Moran's I

We can visualize the test statistics obtained from the simulation above by implementing the hist() and abline() functions of R graphics.

::: panel-tabset
## Summary Statistics

We first calculate the mean and variance, and obtain the summary statistics.

```{r}
mean(bperm$res[1:999])
```

```{r}
var(bperm$res[1:999])
```

```{r}
summary(bperm$res[1:999])
```

## The plot

```{r}
hist(bperm$res, 
     freq=TRUE, 
     breaks=20, 
     xlab="Simulated Moran's I")
abline(v=0, 
       col="red") 
```

From the above, we can infer that over half of all simulations indicate a negative value for Moran’s I statistic. Generally, a negative value indicates that **dissimilar** regions are located next to each other. (i.e: regions with dissimilar number of Accidents are located next to each other)

## ggplot method

We can also make use of the ggplot2 R package to produce a plot.

```{r}
data <- data.frame(simulated_moran = bperm$res)

ggplot(data, aes(x = simulated_moran)) +
  geom_histogram(binwidth = (max(data$simulated_moran) - min(data$simulated_moran)) / 20, 
                 fill = "lightblue", color = "black") +
  geom_vline(xintercept = 0, color = "red", linetype = "dashed") +
  labs(x = "Simulated Moran's I", 
       y = "Frequency",
       title = "Histogram of Simulated Moran's I") +
  theme_minimal()
```
:::

### 1.4.4.3 Local Morans I

We implement the localmoran() function of spdep compute the local Moran’s I statistic. This function helps us compute li values, given a set of zi values and a listw object providing neighbor weighting information for the polygon associated with the zi values.

We compute local Moran’s I of `accident_count` at the district level.

```{r}
fips <- order(boundary_with_accident_count$accident_count)
localMI <- localmoran(boundary_with_accident_count$accident_count, rswm_q)
head(localMI)
```

::: note-box
*localmoran()* function returns a matrix of values whose columns are:

-   Ii: the local Moran’s I statistics

-   E.Ii: the expectation of local moran statistic under the randomisation hypothesis

-   Var.Ii: the variance of local moran statistic under the randomisation hypothesis

-   Z.Ii:the standard deviate of local moran statistic

-   Pr(): the p-value of local moran statistic
:::

We now use the printCoefmat() to display the content of the local Moran matrix that we created above.

```{r}
printCoefmat(data.frame(
  localMI[fips,], 
  row.names=boundary_with_accident_count$ADM2_EN.x[fips]),
  check.names=FALSE)
```

### 1.4.4.4 Mapping the local Moran's I

Before we map the local Moran’s I map, it is wise to append the local Moran’s data-frame (`localMI`) onto our SpatialPolygonDataFrame.

```{r}
bmr.localMI <- cbind(boundary_with_accident_count,localMI) %>%
  rename(Pr.Ii = Pr.z....E.Ii..)
```

We now make use of the tmap package and its choropleth mapping functions to plot the local Moran’s I values.

```{r}
tm_shape(bmr.localMI) +
  tm_fill(col = "Ii", 
          style = "pretty",
          palette = "RdBu",
          title = "local moran statistics") +
  tm_borders(alpha = 0.5)
```

::: insights-box
Interpreting Moran Value:

-   Values lesser than 0 indicate negative spatial autocorrelation. (i.e: high accident areas are near low accident areas.)

-   Value equal to 0 indicates random spatial distribution (No spatial autocorrelation).

-   Values greater than 0 indicate spatial autocorrelation.
:::

### 1.4.4.5 Mapping Local Moran's I p-values

The Choropleth reveals the presence of both positive, as well as negative I values. This indicates that there are varying levels of spatial autocorrelation, however, we must examine the p-values for these I values to check for statistical significance.

We use the tmap package to draw a choropleth map of Moran’s I p-values.

```{r}
tm_shape(bmr.localMI) +
  tm_fill(col = "Pr.Ii", 
          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),
          palette="-Blues", 
          title = "local Moran's I p-values") +
  tm_borders(alpha = 0.5)
```

From the above we can infer that generally the local moran value ranges from -1 to 1. The biggest stand outs are:

1.  Nong Suea, at the top right corner of the above map, seems to be absolutely dissimilar to its neighbors as indicated by its local Moran Value being of the highest magnitude. From our earlier analysis, we infer that this region actually has significantly fewer accidents than its neighbors.

2.  The South-East region of the Bangkok Metropolitan Region has very high local Moran Values, indicating that the districts all showcase similar characteristics. They exhibit spatial clustering.

### 1.4.4.6 Mapping both local Moran's I values and p-values

In the interest of easier analysis and interpretation, we plot the two maps next to each other.

```{r}
localMI.map <- tm_shape(bmr.localMI) +
  tm_fill(col = "Ii", 
          style = "pretty", 
          title = "local moran statistics") +
  tm_borders(alpha = 0.5)

pvalue.map <- tm_shape(bmr.localMI) +
  tm_fill(col = "Pr.Ii", 
          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),
          palette="-Blues", 
          title = "local Moran's I p-values") +
  tm_borders(alpha = 0.5)

tmap_arrange(localMI.map, pvalue.map, asp=1, ncol=2)
```

## 1.4.5 Hot and Cold Spot Areas

### 1.4.5.1 Getis and Ord's G-Statistics

An alternative spatial statistic used to detect spatial anomalies is the **Getis-Ord G-statistic** (Getis and Ord, 1972; Ord and Getis, 1995). This method examines spatial relationships within a defined proximity to identify clusters of high or low values. Statistically significant **hotspots** are areas where high values are spatially clustered, meaning that not only do these areas have high values, but their neighboring areas also exhibit similarly high values.

The analysis involves three key steps:

1.  **Deriving the spatial weight matrix**: This defines the spatial relationships between areas, specifying which locations are considered neighbors based on proximity.

2.  **Computing the Gi statistic**: This step calculates the G-statistic for each location, identifying regions where values are significantly higher or lower than expected.

3.  **Mapping the Gi statistics**: The results are visualized to reveal spatial patterns of high-value clusters (hotspots) and low-value clusters (cold spots).

This approach is useful for identifying localized patterns of spatial clustering and detecting significant anomalies in the data.

### 1.4.5.2 Deriving Distance Based Weight Matrix

We start by defining a new set of neighbors. While the spatial autocorrelation considered units which shared borders, for Getis-Ord, we will define the neighbors based on distance.

::: insights-box
There are two types of distance-based proximity matrices:

1.  Fixed Distance Weight Matrix

2.  Adaptive Distance Weight Matrix
:::

Before creating our connectivity graph, we need to assign a point to each polygon. This requires more than simply running st_centroid() on the us.bound spatial object. Specifically, we need to extract the coordinates into a separate data frame. We have already done this earlier and stored them under the name `coords`.

Mapping functions apply a specific operation to each element of a vector and return a vector of the same length. In our case, the input vector will be the geometry column from us.bound, and the function we’ll apply is st_centroid(). We’ll use the map_dbl variation from the **purrr** package, which is designed to return numeric (double) values.

To extract the longitude values, we’ll map the st_centroid() function over the geometry column and use double bracket notation \[\[\]\] with 1 to access the first element of each centroid, which corresponds to the longitude.

For more detailed information, you can refer to the map documentation [here](https://rdocumentation.org/packages/purrr/versions/1.0.2/topics/map).

#### 1.4.5.2.1 Determine the Cut-off Distance

To determine the upper limit for the distance band, we follow these steps:

1.  **Find the k-nearest neighbors**: Use the `knearneigh()` function from the **spdep** package. This function returns a matrix that contains the indices of points corresponding to the k-nearest neighbors for each observation.

2.  **Convert to a neighbors list**: Take the k-nearest neighbors object returned by `knearneigh()` and convert it into a neighbors list (class `nb`) by using the `knn2nb()` function. This generates a list of integer vectors, where each vector contains the region numbers corresponding to its neighbors.

3.  **Calculate neighbor distances**: Use the `nbdists()` function from **spdep** to calculate the distances between neighbors. The function returns the lengths of neighbor relationship edges in the units of the coordinates (e.g., kilometers if the coordinates are geographic).

4.  **Flatten the distance list**: The distances returned by `nbdists()` are stored in a list. Use the `unlist()` function to remove the list structure and return a single vector of distances.

This process helps identify the upper limit for a distance band by analyzing the distances between neighboring regions.

```{r}
k1 <- knn2nb(knearneigh(coords))
k1dists <- unlist(nbdists(k1, coords, longlat = FALSE))
summary(k1dists)
```

From the above output, we can infer that the largest first-nearest neighbor distance is just under 17000M.\
Using this as the upper threshold gives certainty that all units will have *at least* one neighbor.

#### 1.4.5.2.2 Computing fixed distance weight matrix

We implement the dnearneigh() function of the spdep package to compute the distance weight matrix.

```{r}
wm_d62 <- dnearneigh(coords, 0, 17000, longlat = FALSE)
wm_d62
```

After this, we implement the nb2listw() function to convert the nb object into spatial weights object.

On average, each region has approximately 22.3neighbors.

```{r}
wm62_lw <- nb2listw(wm_d62, style = 'B')
summary(wm62_lw)
```

### 1.4.5.3 Computing Gi statistics using Fixed Distance

```{r}
fips <- order(boundary_with_accident_count$accident_count)
gi.fixed <- localG(boundary_with_accident_count$accident_count, wm62_lw)
gi.fixed
```

The output of the `localG()` function is a vector containing G or G\* values, with the following attributes: - `"gstari"`: Indicates whether the G\* version of the statistic was used (`TRUE` or `FALSE`). - `"call"`: Stores the function call. - `"class"`: Set to `"localG"`, identifying the object type.

The Gi statistic is represented as a Z-score, where larger values signify stronger clustering. The sign of the value indicates the type of cluster: positive values point to high-value clusters (hotspots), while negative values indicate low-value clusters (cold spots).

To merge the Gi values with their corresponding geographic data in the BMR spatial dataframe, use the following code to join the results to the `boundary_with_accident_count` sf object. This allows for the spatial visualization of clusters within the geographic data.

```{r}
bmr.gi <- cbind(boundary_with_accident_count, as.matrix(gi.fixed)) %>%
  rename(gstat_fixed = as.matrix.gi.fixed.)
```

::: note-box
the code chunk above actually performs **three** tasks. First, it convert the output vector (i.e. *gi.fixed*) into r matrix object by using *as.matrix()*. Next, *cbind()* is used to join bmr\@data and *gi.fixed* matrix to produce a new SpatialPolygonDataFrame called *bmr.gi*. Lastly, the field name of the gi values is renamed to *gstat_fixed* by using *rename()*.
:::

We can now map the Gi values derived using the fixed-distance weight matrix.

```{r}
acc_count <- qtm(boundary_with_accident_count, "accident_count")

Gimap <-tm_shape(bmr.gi) +
  tm_fill(col = "gstat_fixed", 
          style = "pretty",
          palette="-RdBu",
          title = "local Gi") +
  tm_borders(alpha = 0.5)

tmap_arrange(acc_count, Gimap, asp=1, ncol=2)
```

From the above plot, we can infer that ‘hot spots’ tend to be neighboring regions and likewise for the cold spots too. We see more high value (hot) clusters in the South East region of the Bangkok Metropolitan Region- Near Samut Prakan. The Central Area, near Bangkok and Nonthaburi showcase more 'cold spots'.

Districts Bang Phli, Bang Sao Thong, Bang Bo, Sinakharin, and Suwinthawong seem to be highly linked to each other based on accidents, all in the South-East region of the Bangkok Metropolitan Region.

# 1.5 Conclusion

From all our analysis, we determine that the South East region of Bangkok Metropolitan Region seems to be a hotspot for accidents and needs to be further examined in order to reduce the rate of accidents in the region.

Speeding fines and laws need to be tightened in order to reduce the number of speeding incidents in the region. It is responsible for over 80% of all road accidents in the region.

Highways in the South-East Region specifically see incidents of drivers going off the road in curvy areas and this is another thing that must be looked at. Barriers and signage must be strengthened in the area.
