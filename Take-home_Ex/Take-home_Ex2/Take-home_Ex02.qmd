---
title: "Take Home Exercise 2- Discovering impacts of COVID-19 on Thailand tourism economy at the province level using spatial and spatio-temporal statistics"
author: "Arjun Singh"
date: 2024-09-30
date-modified: "last-modified"
execute: 
  eval: true
  echo: true
  message: false
  freeze: true
  warning: false
format:
  html:
    css: styles.css 
---

# 2 Introduction

Tourism is one of Thailand’s largest industries, accounting for some 20% of the gross domestic product (GDP). In 2019, Thailand earned 90 billion US\$ from domestic and international tourism, but the COVID-19 pandemic caused revenues to crash to 24 billion US\$ in 2020.

The figure below shows the total revenue for the tourism sector from January 2019 until Feb 2023. The figure reveals that the revenue for the industry have been recovering gradually since September 2021.

![](images/clipboard-378385426.png)

However, it is important to note that the tourism economy of Thailand is not evenly distributed- not all provinces make a lot of revenue.

Thailand has 77 provinces in total as shown on the map below. This map was sourced from [Wikipedia](https://en.wikipedia.org/wiki/Provinces_of_Thailand).

![](images/clipboard-746665905.png)

The figure below reveals that the tourism economy of Thailand is carried by five provinces, namely Bangkok, Phuket, Chiang Mai, Sukhothai and Phetchaburi.

![](images/clipboard-986836251.png)

# 2.1 Objectives

The objectives of this exercise are to understand:

-   if the key indicators of the tourism economy of Thailand are independent from space and space and time.

-   If the tourism economy is indeed spatial and spatio-temporal dependent. If so, then we would like to detect where the clusters and outliers are, as well as the emerging hot spot/cold spot areas.

# 2.2 Data and Packages

The data used for this exercise are as follows:

-   [Thailand Domestic Tourism Statistics](https://www.kaggle.com/datasets/thaweewatboy/thailand-domestic-tourism-statistics) which is sourced from Kaggle. 

    ![](images/Screenshot%202024-10-02%20180644.png)

-   [Thailand - Subnational Administrative Boundaries](https://data.humdata.org/dataset/cod-ab-tha?) which is sourced from Humanitarian Data Exchange. We will use the data at the province level (i.e: ADM1).

    ![](images/clipboard-3292985936.png)

We will be focusing on various variables present in the Thailand Domestic Tourism Statistics dataset. The variables present in the dataset are as follows:

-   `no_tourist_all` The total number of domestic tourists who visited the province

-   `no_tourist_foreign` The number of foreign tourists who visited the province

-   `no_tourist_occupied` The total number of occupied hotel rooms in the province

-   `no_tourist_thai` The number of Thai tourists who visited the province

-   `occupancy_rate` The percentage of occupied travel accommodation in the province

-   `net_profit_all` The net profit generated by the tourism industry in the province, in Thai Baht

-   `net_profit_foreign` The net profit generated by foreign tourists in the province, in Thai Baht

-   `net_profit_thai` The net profit generated by Thai tourists in the province, in Thai Baht

The following packages are used:

-   **`sf`**: Provides simple features support for handling and analyzing spatial vector data in R.
-   **`spdep`**: A package for spatial dependence and spatial regression analysis, particularly for handling spatial weights.
-   **`tmap`**: A flexible visualization package for thematic maps, supporting both static and interactive mapping in R.
-   **`tidyverse`**: A collection of R packages designed for data science, emphasizing data manipulation, visualization, and functional programming.
-   **`knitr`**: A dynamic report generation tool in R, allowing for the integration of code, results, and narrative in reproducible documents.

We now load these packages into our environment by using the p_load() function of the pacman package.

```{r}
pacman::p_load(sf, spdep, tmap, tidyverse, knitr, plotly, Kendall, sfdep)
set.seed(123)
tmap_mode('plot')
```

## 2.1 Importing the data

### 2.1.1 Importing the aspatial data

We will now import the aspatial data by implementing the read_csv() function of the readr package as shown in the code chunk below.

```{r}
tourists=read_csv('data/aspatial/thailand_domestic_tourism_2019_2023.csv')
tourists_temporal= tourists%>%
  mutate(month=month(date))
# Summing the 'value' based on 'province_eng' and 'variable'
summed_data <- tourists %>%
  group_by(province_eng, variable) %>%
  summarize(total_value = sum(value, na.rm = TRUE))

# View the result
head(summed_data)

```

### 2.1.2 Importing the geospatial data

We now import the geospatial data using the st_read() function of the sf package.

```{r}
province=st_read(dsn = "data/geospatial", 
                 layer = "tha_admbnda_adm1_rtsd_20220121")%>%
  select(1:5, 17)
```

We now check the CRS information using the st_crs() function of the sf package and transform the EPSG code using the st_transform() function if it is not 32647, the EPSG code of Thailand.

```{r}
st_crs(province)

st_transform(province, 32647)
```

### 2.1.3 Performing relational join

After performing consistency checks, we notice that the provinces aren't named correctly in our tourist data-set. We correct the names below.

```{r}
summed_data$province_eng <- gsub("Nong Bua Lamphu", "Nong Bua Lam Phu", summed_data$province_eng)
summed_data$province_eng <- gsub("Sisaket", "Si Sa Ket", summed_data$province_eng)
summed_data$province_eng <- gsub("Phang Nga", "Phangnga", summed_data$province_eng)
summed_data$province_eng <- gsub("Lopburi", "Lop Buri", summed_data$province_eng)
summed_data$province_eng <- gsub("Chonburi", "Chon Buri", summed_data$province_eng)
summed_data$province_eng <- gsub("Chainat", "Chai Nat", summed_data$province_eng)
summed_data$province_eng <- gsub("Buriram", "Buri Ram", summed_data$province_eng)
summed_data$province_eng <- gsub("Prachinburi", "Prachin Buri", summed_data$province_eng)
```

We will now join the aspatial and geospatial data by using the left_join() function of the package as showing in the code chunk below.

```{r}
pro_tourism=left_join(summed_data, province, by= c("province_eng"="ADM1_EN"))
```

# 2.3 Exploratory Data Analysis

## 2.3.1 Visualizing regional indicators

After successfully completing the relational join, we can now plot a choropleth map to visualize the tourism in each province in Thailand using various functions of the tmap package.

### 2.3.1.1 Number of tourists

::: panel-tabset

## Number of Foreign Tourists
We first take a look at the number of tourists, both foreign and domestic, across all provinces in Thailand.

```{r}
tourist_foreign=pro_tourism%>%
  filter(variable=='no_tourist_foreign')
tourist_foreign=st_as_sf(tourist_foreign)

tourist_foreign=st_as_sf(tourist_foreign)


# Create the interactive basemap
basemap01 <- tm_shape(tourist_foreign) +
  tm_polygons(col = "total_value", palette = "Blues", style= 'jenks')+
  tm_text("province_eng", size = 0.3) 


# Display the interactive map
basemap01
```

## Number of Domestic Tourists

```{r}
tourist_domestic=pro_tourism%>%
  filter(variable=='no_tourist_thai')

tourist_domestic=st_as_sf(tourist_domestic)


# Create the interactive basemap
basemap02 <- tm_shape(tourist_domestic) +
  tm_polygons(col = "total_value", palette = "Blues", style='jenks')+
  tm_text("province_eng", size = 0.3) 

# Display the interactive map
basemap02
```

## All tourists

```{r}
tourist_all=pro_tourism%>%
  filter(variable=='no_tourist_all')

tourist_all=st_as_sf(tourist_all)


# Create the interactive basemap
basemap03 <- tm_shape(tourist_all) +
  tm_polygons(col = "total_value", palette = "Blues", style='jenks')+
  tm_text("province_eng", size = 0.3) 

# Display the interactive map
basemap03
```
:::

#### 2.3.1.2 Revenue

We now take a look at the revenue generated by foreign and domestic tourists in Thailand

::: panel-tabset
## Revenue generated by Foreigners

```{r}
profit_foreigners=pro_tourism%>%
  filter(variable=='net_profit_foreign')
profit_foreigners=st_as_sf(profit_foreigners)
  


# Create the interactive basemap
basemap <- tm_shape(profit_foreigners) +
  tm_polygons(col = "total_value", palette = "Blues", style= 'jenks') +
  tm_text("province_eng", size = 0.5)

# Display the interactive map
basemap


```

## Revenue generated by Domestic Tourists

```{r}
#| cache: true
profit_domestic=pro_tourism%>%
  filter(variable=='net_profit_thai')

profit_domestic=st_as_sf(profit_domestic)


# Create the interactive basemap
basemap2 <- tm_shape(profit_domestic) +
  tm_polygons(col = "total_value", palette = "Blues", style='jenks') +
  tm_text("province_eng", size = 0.5)

# Display the interactive map
basemap2

```

## Overall Profit

```{r}
profit_all=pro_tourism%>%
  filter(variable=='net_profit_all')
profit_all=st_as_sf(profit_all)
  


# Create the interactive basemap
basemap3 <- tm_shape(profit_all) +
  tm_polygons(col = "total_value", palette = "Blues", style= 'jenks')+
  tm_text("province_eng", size = 0.5)     

# Display the interactive map
basemap3
```

:::

Bangkok seems to be the most lucrative province overall.

#### 2.3.1.3 Temporal Data

##### 2.3.1.3.1 Number of tourists by season

We will now analyse how revenue is distributed based on the month of the year. 

For this, we first aggregate data based on certain features such as holiday periods and seasons.

We determine that there are three distinct seasons in Thailand from the ['UK Meteorological Office'](https://www.metoffice.gov.uk/weather/travel/holiday-weather/asia/thailand#:~:text=Part%20of%20Thailand's%20appeal%20is,season%20(March%20to%20May).  

```{r}
tourists_temporal$province_eng <- gsub("Nong Bua Lamphu", "Nong Bua Lam Phu", tourists_temporal$province_eng)
tourists_temporal$province_eng <- gsub("Sisaket", "Si Sa Ket", tourists_temporal$province_eng)
tourists_temporal$province_eng <- gsub("Phang Nga", "Phangnga", tourists_temporal$province_eng)
tourists_temporal$province_eng <- gsub("Lopburi", "Lop Buri", tourists_temporal$province_eng)
tourists_temporal$province_eng <- gsub("Chonburi", "Chon Buri", tourists_temporal$province_eng)
tourists_temporal$province_eng <- gsub("Chainat", "Chai Nat", tourists_temporal$province_eng)
tourists_temporal$province_eng <- gsub("Buriram", "Buri Ram", tourists_temporal$province_eng)
tourists_temporal$province_eng <- gsub("Prachinburi", "Prachin Buri", tourists_temporal$province_eng)
# Assuming your dataset is called 'tourists_temporal' and has a 'month' column
tourists_temporal <- tourists_temporal %>%
  mutate(season = case_when(
    month %in% c(5, 6, 7, 8, 9, 10) ~ "Wet Season",
    month %in% c(11, 12, 1, 2) ~ "Cool Season",
    month %in% c(3, 4, 5) ~ "Hot Season"
  ))

# View the result
head(tourists_temporal)

```

We now join tourists_temporal df to the province df create separate data-frames for each.

```{r}

# Separate data frames for each season

wet_season_df <- tourists_temporal %>%
  filter(season == "Wet Season")

cool_season_df <- tourists_temporal %>%
  filter(season == "Cool Season")

hot_season_df <- tourists_temporal %>%
  filter(season == "Hot Season")

```

We now sum the values using the group_by() function.

```{r}
# Obtaining aggregates
wet_sum= wet_season_df %>%
  group_by(province_eng, variable) %>%
  summarize(total_value = sum(value, na.rm = TRUE))

# JOINING DF TO THE PROVINCE DF
wet_sum=left_join(wet_sum, province, by=c('province_eng'='ADM1_EN'))

# Obtaining Aggregates
cool_sum= cool_season_df %>%
  group_by(province_eng, variable) %>%
  summarize(total_value = sum(value, na.rm = TRUE))

#Joining to province df
cool_sum= left_join(cool_sum, province, by=c('province_eng'='ADM1_EN'))

# Obtaining Aggregates
hot_sum= hot_season_df %>%
  group_by(province_eng, variable) %>%
  summarize(total_value = sum(value, na.rm = TRUE))

#Joining to province df
hot_sum=left_join(hot_sum, province, by=c('province_eng'='ADM1_EN'))

```
We now create choropleth maps for each by implementing functions of the tmap packages.

::: panel-tabset

## Wet Season

```{r}
wet_sum=st_as_sf(wet_sum)

wet_tourism_foreign=wet_sum%>%filter(variable=='no_tourist_foreign')
wet_tourism_thai= wet_sum%>%filter(variable=='no_tourist_thai')

# Create the basemap
basemap11 <- tm_shape(wet_tourism_foreign) +
  tm_polygons(col = "total_value", palette = "Blues", style='jenks') +
  tm_text("province_eng", size = 0.5)


basemap12<- tm_shape(wet_tourism_thai) +
  tm_polygons(col = "total_value", palette = "Blues", style='jenks') +
  tm_text("province_eng", size = 0.5)

tmap_arrange(basemap11, basemap12)

```
There are far more domestic travellers in the rainy season compared to foreign travellers. 


## Hot season 

```{r}
hot_sum=st_as_sf(hot_sum)

hot_tourism_foreign=hot_sum%>%filter(variable=='no_tourist_foreign')
hot_tourism_thai= hot_sum%>%filter(variable=='no_tourist_thai')

# Create the basemap
basemap13 <- tm_shape(hot_tourism_foreign) +
  tm_polygons(col = "total_value", palette = "Blues", style='jenks') +
  tm_text("province_eng", size = 0.5)


basemap14<- tm_shape(hot_tourism_thai) +
  tm_polygons(col = "total_value", palette = "Blues", style='jenks') +
  tm_text("province_eng", size = 0.5)

tmap_arrange(basemap13, basemap14)
```
## Cool season

```{r}
cool_sum=st_as_sf(cool_sum)

cool_tourism_foreign=cool_sum%>%filter(variable=='no_tourist_foreign')
cool_tourism_thai= cool_sum%>%filter(variable=='no_tourist_thai')

# Create the basemap
basemap15 <- tm_shape(cool_tourism_foreign) +
  tm_polygons(col = "total_value", palette = "Blues", style='jenks') +
  tm_text("province_eng", size = 0.5)


basemap16<- tm_shape(cool_tourism_thai) +
  tm_polygons(col = "total_value", palette = "Blues", style='jenks') +
  tm_text("province_eng", size = 0.5)

tmap_arrange(basemap15, basemap16)
```

:::

We see clear differences in number of tourists across regions based on the season, indicating to us that tourism is not independent from temporal factors such as time of the year.


##### 2.3.1.3.2 Profit from tourists by season

::: panel-tabset

## Wet Season

```{r}
wet_sum=st_as_sf(wet_sum)

wet_profit_foreign=wet_sum%>%filter(variable=='net_profit_foreign')
wet_profit_thai= wet_sum%>%filter(variable=='net_profit_thai')

# Create the basemap
basemap21 <- tm_shape(wet_profit_foreign) +
  tm_polygons(col = "total_value", palette = "Blues", style='jenks') +
  tm_text("province_eng", size = 0.5)


basemap22<- tm_shape(wet_profit_thai) +
  tm_polygons(col = "total_value", palette = "Blues", style='jenks') +
  tm_text("province_eng", size = 0.5)

tmap_arrange(basemap21, basemap22)

```


## Hot season

```{r}
hot_sum=st_as_sf(hot_sum)

hot_profit_foreign=hot_sum%>%filter(variable=='net_profit_foreign')
hot_profit_thai= hot_sum%>%filter(variable=='net_profit_thai')

# Create the basemap
basemap23 <- tm_shape(hot_profit_foreign) +
  tm_polygons(col = "total_value", palette = "Blues", style='jenks') +
  tm_text("province_eng", size = 0.5)


basemap24<- tm_shape(hot_profit_thai) +
  tm_polygons(col = "total_value", palette = "Blues", style='jenks') +
  tm_text("province_eng", size = 0.5)

tmap_arrange(basemap23, basemap24)
```

## Cool Season

```{r}
cool_sum=st_as_sf(cool_sum)

cool_profit_foreign=cool_sum%>%filter(variable=='net_profit_foreign')
cool_profit_thai= cool_sum%>%filter(variable=='net_profit_thai')

# Create the basemap
basemap25 <- tm_shape(cool_profit_foreign) +
  tm_polygons(col = "total_value", palette = "Blues", style='jenks') +
  tm_text("province_eng", size = 0.5)


basemap26<- tm_shape(cool_profit_thai) +
  tm_polygons(col = "total_value", palette = "Blues", style='jenks') +
  tm_text("province_eng", size = 0.5)

tmap_arrange(basemap25, basemap26)
```

:::

We notice that while for the most part, there are significantly more domestic travellers, more profit is earned from foreign travellers throughout the year.

# 2.4 Spatial Analysis

We now implement the **poly2nb()** function of the **spdep** package to compute contiguity weight matrices for the study area selected.

Using this function, we are able to build a ‘neighbors list’ based on regions with contiguous boundaries.

In this function, we will pass an argument, ‘queen’, that can be set as either TRUE (default) or FALSE. If the ‘queen’ argument is not explicitly set to FALSE, the function returns a list of first order neighbors using the Queen criteria.

[You may refer to the `spdep` package documentation here](https://cran.r-project.org/web/packages/spdep/spdep.pdf) to learn more about its functions and arguments.

## 2.4.1 Spatial Weights

### 2.4.1.1 Computing Contiguity Spatial Weights

We use the poly2nb() function as shown in the code chunk below. Using this, we are able to compute a Queen contiguity weight matrix.

::: panel-tabset
## Profit from Foreigners

```{r}
#| eval: false
# Rook contiguity
wm_r <- poly2nb(profit_foreigners, queen=FALSE)
write_rds(wm_r, 'data/rds/wm_r_pro_foreign')

# Queen Contiguity
wm_q <- poly2nb(profit_foreigners, queen=TRUE)
write_rds(wm_q, 'data/rds/wm_q_pro_foreign')
```

::: panel-tabset
## Rook Contiguity

```{r}
wm_r_pro_foreign=read_rds("data/rds/wm_r_pro_foreign")
summary(wm_r_pro_foreign)
```

## Queen Contiguity

```{r}
wm_q_pro_foreign=read_rds("data/rds/wm_q_pro_foreign")
summary(wm_q_pro_foreign)
```
:::

## Domestic Tourists (Profit)

We now repeat the same steps for Domestic tourists.

```{r}
#| eval: false
# Rook contiguity 
wm_r <- poly2nb(profit_domestic, queen=FALSE)
write_rds(wm_r, 'data/rds/wm_r_pro_dom')

# Queen Contiguity
wm_q <- poly2nb(profit_domestic, queen=TRUE)
write_rds(wm_q, 'data/rds/wm_q_pro_dom')
```

We now look at a summary of both using the code chunks below.

::: panel-tabset

## Rook Contiguity

```{r}
wm_r_pro_dom=read_rds("data/rds/wm_r_pro_dom")
summary(wm_r_pro_dom)
```

## Queen Contiguity

```{r}
wm_q_pro_dom=read_rds("data/rds/wm_q_pro_dom")
summary(wm_q_pro_dom)
```
:::

## Tourists (all)

We now check all tourists.

```{r}
#| eval: false
# Rook contiguity
wm_r <- poly2nb(profit_all, queen=FALSE)
write_rds(wm_r, 'data/rds/wm_r_pro_all')

# Queen Contiguity
wm_q <- poly2nb(profit_all, queen=TRUE)
write_rds(wm_q, 'data/rds/wm_q_pro_all')
```

::: panel-tabset

## Rook Contiguity

```{r}
wm_r_pro_all=read_rds("data/rds/wm_r_pro_all")
summary(wm_r_pro_all)
```

## Queen Contiguity

```{r}
wm_q_pro_all=read_rds("data/rds/wm_q_pro_all")
summary(wm_q_pro_all)
```

:::
:::

### 2.4.1.2 Visualizing Contiguity Spatial Weights

A connectivity graph takes a point and displays a line to each neighboring point. We are working with polygons in this situation, so we need to ensure that our points are in order to produce our connectivity graphs.

Usually, the method of choice will be polygon centroids. We calculate using the sf package before moving onto the graphs. Getting latitude and longitude of the Polygon Centroids.

We need points to associate with each polygon before we can make our connectivity graph. It won’t be as simple as applying the st_centroid() function of the sf sf object: *`us.bound`*. We need the coordinates in a separate data-frame for this to work.

To do this, we will use a mapping function which will apply a given function to each element of a vector and returns a vector of the same length. Our input vector will be the geometry column of `us.bound`.

The function that we implement in this situation will be st_centroid().

We will be using the map_dbl variation of map from the purrr package.

::: panel-tabset
## Coordinates of Foreign Travellers

We start by extracting the longitude and latitude values for foreign travellers.

```{r}
longitude_profit_foreign= map_dbl(profit_foreigners$geometry, ~st_centroid(.x)[[1]])
latitude_profit_foreign= map_dbl(profit_foreigners$geometry, ~st_centroid(.x)[[2]])
```

Now that we have the latitude and longitude values, we can use the cbind() function to put the longitude and latitude values into the same object, `coords`.

```{r}
coords_profit_foreign <- cbind(longitude_profit_foreign, latitude_profit_foreign)
```

We use the head() function to verify if `coords` is in the correct format.

```{r}
head(coords_profit_foreign)
```

## Coordinates of Domestic Travellers

```{r}
longitude_profit_domestic= map_dbl(profit_domestic$geometry, ~st_centroid(.x)[[1]])
latitude_profit_domestic= map_dbl(profit_domestic$geometry, ~st_centroid(.x)[[2]])
```

Now that we have the latitude and longitude values, we can use the cbind() function to put the longitude and latitude values into the same object, `coords`. We now create the coords object.

```{r}
coords_profit_domestic=cbind(longitude_profit_domestic, latitude_profit_domestic)
```

We use the head() function to verify if `coords` is in the correct format.

```{r}
head(coords_profit_domestic)
```

## Coordinates for all travellers

```{r}
longitude_profit_all= map_dbl(profit_all$geometry, ~st_centroid(.x)[[1]])
latitude_profit_all= map_dbl(profit_all$geometry, ~st_centroid(.x)[[2]])
```

We now create the coords object.

```{r}
coords_profit_all <- cbind(longitude_profit_all, latitude_profit_all)
head(coords_profit_all)
```
:::

We can now visualize it using the plot() function as shown in the following code chunks.

::: panel-tabset
## Profits from foreign travellers

```{r}
plot(profit_foreigners$geometry, border="lightgrey")
plot(wm_r_pro_foreign, coords_profit_foreign, pch = 19, cex = 0.6, add = TRUE, col = "purple")
```

```{r}
plot(profit_foreigners$geometry, border="lightgrey")
plot(wm_q_pro_foreign, coords_profit_foreign, pch = 19, cex = 0.6, add = TRUE, col = "purple")
```

## Profits from domestic travellers

```{r}
plot(profit_domestic$geometry, border="lightgrey")
plot(wm_r_pro_dom, coords_profit_domestic, pch = 19, cex = 0.6, add = TRUE, col = "purple")

```

```{r}
plot(profit_domestic$geometry, border="lightgrey")
plot(wm_q_pro_dom, coords_profit_foreign, pch = 19, cex = 0.6, add = TRUE, col = "purple")
```

## Profits from all travellers

```{r}
plot(profit_all$geometry, border="lightgrey")
plot(wm_r_pro_all, coords_profit_all, pch = 19, cex = 0.6, add = TRUE, col = "purple")
```

```{r}
plot(profit_all$geometry, border="lightgrey")
plot(wm_q_pro_all, coords_profit_foreign, pch = 19, cex = 0.6, add = TRUE, col = "purple")
```
:::

### 2.4.1.3 Computing Distance Based Neighbors

In order to derive distance-based weight matrices, we will implement the **dnearneigh()** function of the **spdep** package.

This function identifies neighbors of region points by Euclidean Distance with a distance band with lower d1 and upper d2 bounds controlled by the `bounds=` argument.

If un-projected coordinates are used and either specified in the coordinates object x or with x as a two column matrix and `longlat=TRUE,` great circle distances in km will be calculated assuming the WGS84 reference ellipsoid.

#### 2.4.1.3.1 Determining cut-off distance

We must first determine the upper limit for the distance band by using the steps shown below:

-   **Find k Nearest Neighbours**: Use **`knearneigh()`** from the **`spdep`** package to get a matrix of indices for the k nearest neighbours of each point.

-   **Convert to Neighbours List**: Convert the **`knn`** object returned by **`knearneigh()`** into a neighbours list of class **`nb`** using **`knn2nb()`**. This list contains integer vectors with neighbour region number IDs.

-   **Calculate Edge Lengths**: Use **`nbdists()`** from **`spdep`** to return the lengths of neighbour relationship edges. The function returns distances in the units of the coordinates if projected, otherwise in kilometers.

-   **Flatten the List**: Remove the list structure of the returned object using **`unlist()`**

We focus on ALL travellers as opposed to singling out Foreign and/or Domestic travellers for the following.

```{r}
k1_pro_all <- knn2nb(knearneigh(coords_profit_all))
k1dists_pro_all <- unlist(nbdists(k1_pro_all, coords_profit_all, longlat = TRUE))
summary(k1dists_pro_all)
```

::: note-box
You could also do the below if you are interested in singling them out based on foreign and domestic travellers.
:::

::: panel-tabset
## Profits from foreign travellers

```{r}
#| eval: false
k1_pro_foreign <- knn2nb(knearneigh(coords_profit_foreign))
k1dists_pro_foreign <- unlist(nbdists(k1_pro_foreign, coords_profit_foreign, longlat = TRUE))
summary(k1dists_pro_foreign)
```

From the output above, we can infer that the largest first nearest neighbor distance is just under 125KM. Using this value, 125KM, as the upper threshold gives certainty that all units will have at least one neighbor.

## Profits from domestic travellers

```{r}
#| eval: false
k1_pro_domestic <- knn2nb(knearneigh(coords_profit_domestic))
k1dists_pro_domestic <- unlist(nbdists(k1_pro_domestic, coords_profit_domestic, longlat = TRUE))
summary(k1dists_pro_domestic)
```
:::

#### 2.4.1.3.2 Computing Distance Based Weight Matrix

We now implement the dnearneigh() function to compute the distance weight matrix.

```{r}
wm_d62_pro_all <- dnearneigh(coords_profit_all, 0, 111, longlat = TRUE)
wm_d62_pro_all
```

::: insights-box
From the output above, we infer that there are 69 distinct regions, as we identified earlier. There are 368 connections between regions where the distance is within the threshold that we have set. 7.73% of all possible region pairs have a connection. On average, each region is connected to approximately 5.3 other regions.
:::

We now use the combination of table() and card() functions from the spdep package to display the structure of the weight matrix.

```{r}
table(profit_all$province_eng, card(wm_d62_pro_all))
```

Next, we implement the n.comp.nb() function to identify the number of connected components in a neighbor list object of class nb.

::: note-box
Note: A connected component is a subset of regions where each region is reachable from any other region within the same subset. The function returns an object that includes the number of connected components (`nc`) and a vector indicating the component membership for each region.
:::

```{r}
n_comp_pro_all<- n.comp.nb(wm_d62_pro_all)
n_comp_pro_all$nc
```

```{r}
table(n_comp_pro_all$comp.id)
```

#### 2.4.1.3.3 **Plotting fixed distance weight matrix**

We now plot the distance weight matrix using the plot() function.

```{r}
plot(profit_all$geometry, border="lightgrey")
plot(wm_d62_pro_all, coords_profit_all, add=TRUE)
plot(k1_pro_all, coords_profit_all, add=TRUE, col="purple", length=0.08)
```

As identified earlier, we see two distinct groups. The upper 63 and the bottom 14.

```{r}
par(mfrow=c(1,2))
plot(profit_all$geometry, border="lightgrey", main="1st nearest neighbours")
plot(k1_pro_all, coords_profit_foreign, add=TRUE, col="red", length=0.08)
plot(profit_all$geometry, border="lightgrey", main="Distance link")
plot(wm_d62_pro_all, coords_profit_foreign, add=TRUE, pch = 19, cex = 0.6)
```

### 2.4.1.4 **Weights based on Inversed Distance Weighting (IDW)**

We first compute the distances between areas by implementing the **nbdists()** function of the **spdep** package.

```{r}
dist_pro_all <- nbdists(wm_q_pro_all, coords_profit_all, longlat = TRUE)
ids_pro_all <- lapply(dist_pro_all, function(x) 1/(x))
ids_pro_all
```

### 2.4.1.5 **Row-Standardized Weights Matrix**

We now need to assign weights to each neighboring polygon. We use equal weights (style=“W”), where each neighboring polygon is assigned a weight of 1 divided by the number of neighbors.

This means each neighboring county’s weight is calculated as 1/(# of neighbors), and these weights are then used to sum the weighted income values.

While this method is intuitive for summarizing neighbors’ values, it has a drawback: polygons at the edges of the study area may rely on fewer neighbors, potentially skewing the spatial autocorrelation results.

::: note-box
Note: For simplicity, we’ll use the style=“W” option in this example, but be aware that more robust options, such as style=“B”, are available.
:::

```{r}
rswm_q_pro_all <- nb2listw(wm_q_pro_all, style="W", zero.policy = TRUE)
rswm_q_pro_all
```

Setting the argument `zero.policy` to TRUE allows for lists of non-neighbors. This should be used with caution as users may not be aware of missing neighbors in their data however setting `zero,policy` to FALSE would return an error.

::: insights-box
The `nb2listw()` function requires an input of class `nb`, representing a neighborhood object. The function’s two key arguments are `style` and `zero.policy`.

-   The `style` argument defines how the weights are calculated. It can take several values:

    -   `"B"`: Binary coding, where weights are either 0 or 1.

    -   `"W"`: Row-standardized, where the sum of weights across all neighbors equals 1.

    -   `"C"`: Globally standardized, where the sum of weights across all neighbors equals the total number of neighbors.

    -   `"U"`: A variation of `"C"`, where weights are normalized by the number of neighbors.

    -   `"S"`: A variance-stabilizing scheme proposed by Tiefelsdorf et al. (1999), which adjusts weights based on the number of neighbors.

-   The `zero.policy` argument, when set to `TRUE`, handles regions with no neighbors by assigning them a weight vector of zero length. This results in a spatial lag value of zero for regions without neighbors, which may or may not be a suitable assumption depending on the context. For such regions, the spatially lagged value is computed as the sum of the products of a zero vector with any numerical vector `x`, effectively setting the lagged value to zero for those regions.
:::

The code chunk below is implemented to check the weights of the first polygons three neighbors type:

```{r}
rswm_q_pro_all$weights[10]
```

::: insights-box
Each neighbor is assigned a 0.33 of the total weight. This means that when R computes the average neighboring income values, each neighbor’s income will be multiplied by 0.125 before being tallied.
:::

Using the same method, we derive a row standardized distance weight matrix by using the code chunk below.

```{r}
rswm_ids_pro_all <- nb2listw(wm_q_pro_all, glist=ids_pro_all, style="B", zero.policy=TRUE)
rswm_ids_pro_all

```

### 2.4.1.6 Application of Spatial Weight Matrix

We now create four different spatial lagged variables:

-   spatial lag with row-standardized weights

-   spatial lag as a sum of neighbouring values

-   spatial window average

-   spatial window sum

#### 2.4.1.6.1 **Spatial Lag With Row-Standardized Weights**

We now compute the average neighbor profit value for each polygon. We often refer to these values as Spatially Lagged Values.

```{r}
pro_all.lag <- lag.listw(rswm_q_pro_all, profit_all$total_value)
pro_all.lag
```

We can append the spatially lagged profit values onto our `profit_foreigners` sf data-frame by using the code chunk shown below.

```{r}
lag.list_pro_all <- list(profit_all$province_eng, lag.listw(rswm_q_pro_all, profit_all$total_value))
lag.res_pro_all <- as.data.frame(lag.list_pro_all)
colnames(lag.res_pro_all) <- c("province_eng", "lag Profit")
profit_all <- left_join(profit_all,lag.res_pro_all)
```

We now plot the actual profit and spatial lag profits side by side to facilitate comparison.

```{r}
pro_all <- qtm(profit_all, "total_value")
lag_pro_all <- qtm(profit_all, "lag Profit")
tmap_arrange(pro_all, lag_pro_all, asp=1, ncol=2)
```

We see a difference in the surrounding regions of Bangkok as well as Mae Hong Son, all of which are in a higher band as compared to the non-spatially lagged values.

#### 2.4.1.6.2 **Spatial Window Sum**

The spatial window sum is the counter part of the window average, but without using row-standardized weights.

To add the diagonal element to the neighbour list, we just need to use *include.self()* from **spdep**.

```{r}
wm_qs_profit <- include.self(wm_q_pro_all)
wm_qs_profit
```

We now assign binary weights to the neighbour structure that includes the diagonal element.

```{r}
b_weights <- lapply(wm_qs_profit, function(x) 0*x + 1)
b_weights[1]
```

Notice that now \[1\] has four neighbours instead of three.

Again, we use *nb2listw()* and *glist()* to explicitly assign weight values.

```{r}
b_weights2 <- nb2listw(wm_qs_profit, 
                       glist = b_weights, 
                       style = "B")
b_weights2
```

With our newly obtained weight structure, we can compute the lag variable with *lag.listw()*.

```{r}
w_sum_profit <- list(profit_all$province_eng, lag.listw(b_weights2, profit_all$total_value))
w_sum_profit
```

Next, we will convert the lag variable listw object into a data.frame by using *as.data.frame()*.

```{r}
w_sum_profit.res <- as.data.frame(w_sum_profit)
colnames(w_sum_profit.res) <- c("province_eng", "w_sum Profit")
```

::: note-box
Do note that the second command line on the code chunk above renames the field names of *w_sum_profit.res* object into province_eng and *w_sum Profit* respectively.
:::

Next, the code chunk below will be used to append w_sum *Profit* values onto our profit sf data.frame by using left_join() of **dplyr** package.

```{r}
profit_all <- left_join(profit_all, w_sum_profit.res)
```

To compare the values of lag Profit and Spatial window average, the `kable()` function of the Knitr package is used to prepare a table using the code chunk below.

```{r}
profit_all %>%
  select("province_eng", "total_value", "w_sum Profit") %>%
  kable()
```

We now plot the actual profit and w_sum_profit maps next to each other using the qtm() function of the tmap package.

```{r}
# Create the first map for 'total_value' using Jenks classification
profit_map <- tm_shape(profit_all) +
              tm_polygons("total_value", style = "jenks", palette = "Blues", title = "Total Value") +
              tm_layout(legend.outside = TRUE)

# Create the second map for 'w_sum Profit' using Jenks classification
w_sum_profit_map <- tm_shape(profit_all) +
                    tm_polygons("w_sum Profit", style = "jenks", palette = "Reds", title = "W Sum Profit") +
                    tm_layout(legend.outside = TRUE)
tmap_arrange(profit_map, w_sum_profit_map, ncol = 2)

```

## 2.4.2 Global Measures of Spatial Autocorrelation

::: panel-tabset
### 2.4.2.1 Maron's I test

We now conduct Moran’s I statistics testing by using the **moran.test()** function of the **spdep** package.

::: note-box
Statistical tests are conducted at a 5% significance level.
:::

The hypotheses for the test are as follows:

-   H0: Regions with similar levels of profit from tourism are randomly distributed.

-   H1: Regions with similar levels of profit from tourism are not randomly distributed and exhibit spatial clustering.

```{r}
moran.test(profit_all$total_value, 
           listw=rswm_q_pro_all, 
           zero.policy = TRUE, 
           na.action=na.omit)
```

From the output above, we can infer the following:

-   The p-value 0.7224\>0.05, indicating that the observed spatial autocorrelation is not statistically significant.

-   Moran’s I statistic: The observed value of -0.037 indicates **no spatial autocorrelation**, meaning that regions with similar levels of profit from tourism are randomly distributed.

There isn't sufficient evidence to reject H0 and we conclude that there is no spatial clustering with regards to profits from tourism in Thailand.

::: note-box
If Morans I Statistic is = 0, there is Complete Random Spatial Distribution.
:::

#### 2.4.2.1.1 Monte Carlo Moran's I

We now implement the moran.mc() function of the spdep package. In this scenario, we will run 1000 simulations.

```{r}
bperm= moran.mc(profit_all$total_value, 
           listw=rswm_q_pro_all, 
                nsim=999, 
                zero.policy = TRUE, 
                na.action=na.omit)
bperm
```

::: insights-box
From the above output, notice that the observed rank is 326. This indicates that the observed Moran's I value of -0.037893 is not 'unusual' compared to the distribution that was generated by the simulations run. This further supports the high p-value and reinforces our earlier conclusion that there is no significant spatial autocorrelation in the data.
:::

We visualize the test statistics obtained from the above simulation by implementing the below code chunk.

::: panel-tabset
## Summary Statistics

```{r}
# Mean
mean(bperm$res[1:999])

# Variance
var(bperm$res[1:999])

# Summary
summary(bperm$res[1:999])
```

## The plot

```{r}
hist(bperm$res, 
     freq=TRUE, 
     breaks=20, 
     xlab="Simulated Moran's I")
abline(v=0, 
       col="red") 
```
:::

### 2.4.2.2 Geary's C

We now implement a further test to verify if our findings from the above test are indeed correct.

The Geary’s C test for spatial autocorrelation is implemented by using the **geary.test()** function of the **spdep** package.

```{r}
geary.test(profit_all$total_value, listw=rswm_q_pro_all)
```

We once again see that the p-value (0.8784) is greater than 0.05. We do not have sufficient evidence to reject the null hypothesis. In fact, from the Geary C statistic value, we infer that there is actually **negative** spatial autocorrelation.

#### 2.4.2.2.1 Monte Carlo Geary's C

We implement the the geary.mc() function of the spdep package to conduct 1000 simulations.

```{r}
bperm=geary.mc(profit_all$total_value, listw=rswm_q_pro_all, 
               nsim=999)
bperm
```

The simulations above reinforce our earlier conclusion.
:::

### 2.4.2.3 Spatial Correlogram

Spatial correlograms are a powerful tool for analyzing patterns of spatial autocorrelation in your data or model residuals. They illustrate how the correlation between pairs of spatial observations changes as the distance (or lag) between them increases. Essentially, they plot an index of autocorrelation, such as Moran’s I or Geary’s C, against distance.

While correlograms are not as central to geostatistics as variograms—an essential concept in that field—they offer valuable insights as an exploratory and descriptive tool. In fact, for examining spatial autocorrelation, correlograms often provide more detailed information than variograms, making them particularly useful for initial spatial data analysis.

::: panel-tabset
## Moran's I Correlogram

We implement the sp.correlogram() function of the spdep package to compute a 6-lag spatial correlogram of profit from tourism in Thailand. The global spatial autocorrelation used in Moran’s I.

The **plot()** of base Graph is then used to plot the output.

```{r}
MI_corr <- sp.correlogram(wm_q_pro_all, 
                          profit_all$total_value, 
                          order=6, 
                          method="I", 
                          style="W", zero.policy = TRUE)
plot(MI_corr)
```

The plot above may not allow us to provide complete interpretation. This is because not all autocorrelation values are statistically significant. Hence, it is important for us to examine the full analysis report by printing out the analysis results as in the code chunk below.

```{r}
print(MI_corr)
```

From above, we can conclude that there is NO spatial autocorrelation.

## Geary's C Correlogram

We implement the `sp.correlogram()` of **spdep** package is used to compute a 6-lag spatial correlogram of total_value, profit. The global spatial autocorrelation used in Geary’s C. The **plot()** of base Graph is then used to plot the output.

```{r}
GC_corr <- sp.correlogram(wm_q_pro_all, 
                          profit_all$total_value, 
                          order=6, 
                          method="C", 
                          style="W", zero.policy = TRUE)
plot(GC_corr)
```

Similar to the step done for Moran's I, we will print out the analysis report by using the code chunk below.

```{r}
print(GC_corr)
```

Indeed, our findings are reinforced.
:::

## 2.4.3 Local Indicators of Spatial Association

Local Indicators of Spatial Association or LISA are statistics that evaluate the existence of clusters and/or outliers in the spatial arrangement of a given variable. For instance if we are studying distribution of profits from tourism across Thailand, local clusters in profit mean that there are counties that have higher or lower rates than is to be expected by chance alone; that is, the values occurring are above or below those of a random distribution in space.

### 2.4.3.1 Computing Local Moran's I

We implement the localmoran() function of spdep compute the local Moran’s I statistic. This function helps us compute li values, given a set of zi values and a listw object providing neighbor weighting information for the polygon associated with the zi values.

We compute Local Moran's I for Profits from Tourism at the County Level

```{r}
fips <- order(profit_all$province_eng)
localMI <- localmoran(profit_all$total_value, rswm_q_pro_all)
head(localMI)
```

::: insights-box
*localmoran()* function returns a matrix of values whose columns are:

-   Ii: the local Moran’s I statistics

-   E.Ii: the expectation of local moran statistic under the randomisation hypothesis

-   Var.Ii: the variance of local moran statistic under the randomisation hypothesis

-   Z.Ii:the standard deviate of local moran statistic

-   Pr(): the p-value of local moran statistic
:::

We now use the printCoefmat() to display the content of the local Moran matrix that we created.

```{r}
printCoefmat(data.frame(
  localMI[fips,], 
  row.names=profit_all$province_eng[fips]),
  check.names=FALSE)
```

#### 2.4.3.1.1 Mapping the Local Moran's I

Before we map the local Moran’s I map, it is wise to append the local Moran’s data-frame (`localMI`) onto the profit SpatialPolygonDataFrame.

```{r}
profit.localMI <- cbind(profit_all,localMI) %>%
  rename(Pr.Ii = Pr.z....E.Ii..)
```

::: panel-tabset
## Mapping the Local Moran's I

We now make use of the tmap package and its choropleth mapping functions to plot the local Moran’s I values.

```{r}
tm_shape(profit.localMI) +
  tm_fill(col = "Ii", 
          style = "pretty",
          palette = "RdBu",
          title = "local moran statistics") +
  tm_borders(alpha = 0.5)
```

## Mapping Local Moran's P-values

The Choropleth reveals the presence of both positive, as well as negative I values. This indicates that there are varying levels of spatial autocorrelation, however, we must examine the p-values for these I values to check for statistical significance.

We use the tmap package to draw a choropleth map of Moran’s I p-values.

```{r}
tm_shape(profit.localMI) +
  tm_fill(col = "Pr.Ii", 
          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),
          palette="-Blues", 
          title = "local Moran's I p-values") +
  tm_borders(alpha = 0.5)

```
:::

We map both of the above plots side by side for comparison.

```{r}
localMI.map <- tm_shape(profit.localMI) +
  tm_fill(col = "Ii", 
          style = "pretty", 
          title = "local moran statistics") +
  tm_borders(alpha = 0.5)

pvalue.map <- tm_shape(profit.localMI) +
  tm_fill(col = "Pr.Ii", 
          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),
          palette="-Blues", 
          title = "local Moran's I p-values") +
  tm_borders(alpha = 0.5)

tmap_arrange(localMI.map, pvalue.map, asp=1, ncol=2)
```

### 2.4.3.2 Creating a LISA Cluster Map

The LISA Cluster Map shows the significant locations color coded by type of spatial autocorrelation.

Before we can generate the LISA cluster map, we must plot the Moran scatterplot.

#### 2.4.3.2.1 Plotting a Moran Scatterplot

The Moran Scatterplot depicts the relationship between the values of the chosen attribute at each location and the average value of the same attribute at neighboring locations.

We will implement the moran.plot() function of the spdep package to create the plot.

```{r}
nci <- moran.plot(profit_all$total_value, rswm_q_pro_all,
                  labels=as.character(profit_all$province_eng), 
                  xlab="Total Value", 
                  ylab="Spatially Lag Total Value")
```

::: insights-box
Notice that the plot is split in 4 quadrants.

The top right corner belongs to areas that have high profits and are surrounded by other areas that have the average level of profits.
:::

#### 2.4.3.2.2 Plotting Moran Scatterplot with Standardised variable

We first implement the scale() function to center and scale the variable. Here, centering is done by subtracting the mean (omitting NAs) from the corresponding columns, and scaling is done by dividing the (centered) variable by their standard deviations.

```{r}
profit_all$Z.value <- scale(profit_all$total_value) %>% 
  as.vector 

```

::: note-box
Note that the as.vector() function is added so that we get a vector as the output. This allows us to map it neatly into our data-frame.
:::

We can now plot the Moran scatterplot again by using the code chunk below.

```{r}
nci2 <- moran.plot(profit_all$Z.value, rswm_q_pro_all,
                   labels=as.character(profit_all$province_eng),
                   xlab="z-Value", 
                   ylab="Spatially Lag z-Value")
```

#### 2.4.3.2.3 Preparing LISA Map Classes

We now prepare the data in order to facilitate plotting a LISA Cluster Map.

```{r}
quadrant <- vector(mode="numeric",length=nrow(localMI))

```

Now, we will derive the spatially lagged variable of interest (i.e: GDPPC) and center the spatially lagged variable around its mean.

```{r}
profit_all$`lag Profit` <- lag.listw(rswm_q_pro_all, profit_all$total_value)
DV <- profit_all$`lag Profit` - mean(profit_all$`lag Profit`)     

```

```{r}
# Now, we work on centering the local Moran around the mean.
LM_I <- localMI[,1]   

# We set the significance level for the Local Moran in the code chunk below.
signif <- 0.05       

# The following code chunk defines the four categories (low-low (1), low-high (2), high-low (3), high-high (4))
quadrant[DV <0 & LM_I>0] <- 1
quadrant[DV >0 & LM_I<0] <- 2
quadrant[DV <0 & LM_I<0] <- 3  
quadrant[DV >0 & LM_I>0] <- 4   

# Finally, we place the non-significant Moran in the category 0.
quadrant[localMI[,5]>signif] <- 0

```

#### 2.4.3.2.3 LISA Map

We can now implement functions of the tmap package to plot the LISA Map.

```{r}
profit.localMI$quadrant <- quadrant
colors <- c("#ffffff", "#2c7bb6", "#abd9e9", "#fdae61", "#d7191c")
clusters <- c("insignificant", "low-low", "low-high", "high-low", "high-high")

tm_shape(profit.localMI) +
  tm_fill(col = "quadrant", 
          style = "cat", 
          palette = colors[c(sort(unique(quadrant)))+1], 
          labels = clusters[c(sort(unique(quadrant)))+1],
          popup.vars = c("")) +
  tm_view(set.zoom.limits = c(11,17)) +
  tm_borders(alpha=0.5)
```

In the interest of easier visualization and interpretation, we plot the Total Values and their corresponding quadrants next to each other.

```{r}
profits <- tm_shape(profit_all) +
              tm_polygons("total_value", style = "jenks", palette = "Blues", title = "Total Value") +
              tm_layout(legend.outside = TRUE)

profit.localMI$quadrant <- quadrant
colors <- c("#ffffff", "#2c7bb6", "#abd9e9", "#fdae61", "#d7191c")
clusters <- c("insignificant", "low-low", "low-high", "high-low", "high-high")

LISAmap <- tm_shape(profit.localMI) +
  tm_fill(col = "quadrant", 
          style = "cat", 
          palette = colors[c(sort(unique(quadrant)))+1], 
          labels = clusters[c(sort(unique(quadrant)))+1],
          popup.vars = c("")) +
  tm_view(set.zoom.limits = c(11,17)) +
  tm_borders(alpha=0.5)

tmap_arrange(profits, LISAmap, 
             asp=1, ncol=2)
```

### 2.4.3.3 Hot and Cold Spot Analysis

We now use localized spatial statistics to detect hot and cold spot areas.

::: note-box
“Hot Spot’ is generally used across various disciplines to describe a region or value that is higher relative to its surroundings.
:::

```{r}

```

#### 2.4.3.3.1 Getis and Ord’s G-Statistics

An alternative spatial statistic used to detect spatial anomalies is the Getis-Ord G-statistic (Getis and Ord, 1972; Ord and Getis, 1995). This method examines spatial relationships within a defined proximity to identify clusters of high or low values. Statistically significant hotspots are areas where high values are spatially clustered, meaning that not only do these areas have high values, but their neighboring areas also exhibit similarly high values.

The analysis involves three key steps:

Deriving the spatial weight matrix: This defines the spatial relationships between areas, specifying which locations are considered neighbors based on proximity. Computing the Gi statistic: This step calculates the G-statistic for each location, identifying regions where values are significantly higher or lower than expected. Mapping the Gi statistics: The results are visualized to reveal spatial patterns of high-value clusters (hotspots) and low-value clusters (cold spots). This approach is useful for identifying localized patterns of spatial clustering and detecting significant anomalies in the data.

#### 2.4.3.3.2 Deriving Distance Based Weight Matrix

We start by defining a new set of neighbors. While the spatial autocorrelation considered units which shared borders, for Getis-Ord, we will define the neighbors based on distance.

There are two types of distance-based proximity matrices:

Fixed Distance Weight Matrix

Adaptive Distance Weight Matrix

##### 2.4.3.3.2.1 Deriving distance-based weight matrix

::: panel-tabset
## fixed distance matrix

Before creating our connectivity graph, we need to assign a point to each polygon. This requires more than simply running st_centroid() on the us.bound spatial object. Specifically, we need to extract the coordinates into a separate data frame. We have already done this previously and have the object coords_all

For more detailed information, you can refer to the map documentation here.

```{r}
wm62_lw_profit <- nb2listw(wm_d62_pro_all, style = 'B')
summary(wm62_lw_profit)
```

```{r}
fips <- order(profit_all$province_eng)
gi.fixed_profit <- localG(profit_all$total_value, wm62_lw_profit)
gi.fixed_profit
```

The output of the localG() function is a vector containing G or G\* values, with the following attributes: - "gstari": Indicates whether the G\* version of the statistic was used (TRUE or FALSE). - "call": Stores the function call. - "class": Set to "localG", identifying the object type.

The Gi statistic is represented as a Z-score, where larger values signify stronger clustering. The sign of the value indicates the type of cluster: positive values point to high-value clusters (hotspots), while negative values indicate low-value clusters (cold spots).

To merge the Gi values with their corresponding geographic data in the Hunan spatial dataframe, use the following code to join the results to the profit sf object. This allows for the spatial visualization of clusters within the geographic data.

```{r}
profit.gi <- cbind(profit_all, as.matrix(gi.fixed_profit)) %>%
  rename(gstat_fixed = as.matrix.gi.fixed_profit.)
```

the code chunk above actually performs three tasks. First, it convert the output vector (i.e. gi.fixed) into r matrix object by using as.matrix(). Next, cbind() is used to join hunan\@data and gi.fixed matrix to produce a new SpatialPolygonDataFrame called hunan.gi. Lastly, the field name of the gi values is renamed to gstat_fixed by using rename().

## Mapping Gi values with fixed distance weights

```{r}
Gimap <-tm_shape(profit.gi) +
  tm_fill(col = "gstat_fixed", 
          style = "pretty",
          palette="-RdBu",
          title = "local Gi") +
  tm_borders(alpha = 0.5)

tmap_arrange(profits, Gimap, asp=1, ncol=2)
```

From the above plot, we can infer that ‘hot spots’ tend to be neighboring regions and likewise for the cold spots too. We see high value (hot) clusters in the Central region of Thailand, particularly around Bangkok and the Bangkok Metropolitan Region as well as in the Southern Region, while the majority of the western part of Thailand is ‘cold’.
:::

#### 2.4.3.3.2 Emerging Hot-Spot Analysis

We will now conduct Emerging Hot-spot analysis to see if there are any trends popping up recently.

Emerging Hot Spot Analysis (EHSA) is a spatio-temporal analysis method for revealing and describing how hot spot and cold spot areas evolve over time. The analysis consist of four main steps:

-   Building a space-time cube,

-   Calculating Getis-Ord local Gi\* statistic for each bin by using an FDR correction,

-   Evaluating these hot and cold spot trends by using Mann-Kendall trend test,

-   Categorising each study area location by referring to the resultant trend z-score and p-value for each location with data, and with the hot spot z-score and p-value for each bin.

##### 2.4.3.3.2.1 Space-Time Cube

We now use the [`spacetime()`](https://sfdep.josiahparry.com/reference/spacetime.html) function of the sfdep package to create a spatio-temporal cube.

<details>

<summary>Click to show/hide code</summary>

```{r}
# Applying the province name changes to the tourists dataset
tourists_temporal$province_eng <- gsub("Nong Bua Lamphu", "Nong Bua Lam Phu", tourists_temporal$province_eng)
tourists_temporal$province_eng <- gsub("Sisaket", "Si Sa Ket", tourists_temporal$province_eng)
tourists_temporal$province_eng <- gsub("Phang Nga", "Phangnga", tourists_temporal$province_eng)
tourists_temporal$province_eng <- gsub("Lopburi", "Lop Buri", tourists_temporal$province_eng)
tourists_temporal$province_eng <- gsub("Chonburi", "Chon Buri", tourists_temporal$province_eng)
tourists_temporal$province_eng <- gsub("Chainat", "Chai Nat", tourists_temporal$province_eng)
tourists_temporal$province_eng <- gsub("Buriram", "Buri Ram", tourists_temporal$province_eng)
tourists_temporal$province_eng <- gsub("Prachinburi", "Prachin Buri", tourists_temporal$province_eng)

```

</details>


```{r}
profit_all_temporal=tourists_temporal%>%filter(variable=='net_profit_all')

# Rename province_eng to ADM1_EN using base R in order to facilitate the creation of the spacetime cube.
names(profit_all_temporal)[names(profit_all_temporal) == "province_eng"] <- "ADM1_EN"

# Space-time by month
profits_st <- spacetime(profit_all_temporal, province,
                      .loc_col = "ADM1_EN",
                      .time_col = "date")

```

To verify if the cube has been create, we implement the is_spacetime_cube() function of the sfdep package as shown in the code chunk below.

```{r}
is_spacetime_cube(profits_st)
```
Based on the above output, we can confirm that it has been created as intended.


##### 2.4.3.3.2.2 Deriving Spatial Weights

We implement the below code chunk to identify neighbors and calculate the inverse-distance weights.

```{r}
profits_nb <- profits_st %>%
  activate("geometry") %>%
  mutate(nb = include_self(
    st_contiguity(geometry)),
    wt = st_inverse_distance(nb, 
                             geometry, 
                             scale = 1,
                             alpha = 1),
    .before = 1) %>%
  set_nbs("nb") %>%
  set_wts("wt")
```

::: insights-box
-   `activate()` of dplyr package is used to activate the geometry context

-   `mutate()` of dplyr package is used to create two new columns *nb* and *wt*.

-   Then we will activate the data context again and copy over the nb and wt columns to each time-slice using `set_nbs()` and `set_wts()`

    -   row order is very important so do not rearrange the observations after using `set_nbs()` or `set_wts()`.
:::

##### 2.4.3.3.3 Computing Gi Stats

We can use these new columns to manually calculate the local Gi for each location. We do this by grouping by *Year* and using the `local_gstar_perm()` function of the sfdep package. After this, we use the `unnest()` function to unnest the *gi_star* column of the newly created *gi_starts* data.frame.

```{r}
# Calculate the local G* statistics
gi_stars <- profits_nb %>%
  group_by(date) %>%
  mutate(gi_star = local_gstar_perm(
    value,  # Make sure it's numeric
    nb,                   # Neighbors
    wt                    # Weights
  )) %>%
  tidyr::unnest(gi_star)

```

##### 2.4.3.3.4 **Mann-Kendall Test**

With the above Gi calculations, we can now conduct the Mann-Kendall test to evaluate selected locations, the top 5 most lucrative regions, for a trend.

::: panel-tabset

## Bangkok

```{r}
bkk <- gi_stars %>% 
  ungroup() %>% 
  filter(ADM1_EN == "Bangkok") %>% 
  select(ADM1_EN, date, gi_star)
```

We can now produce a plot by using the ggplot package.

```{r}
ggplot(data = bkk, 
       aes(x = date, 
           y = gi_star)) +
  geom_line() +
  theme_light()
```
## Krabi

```{r}
kra <- gi_stars %>% 
  ungroup() %>% 
  filter(ADM1_EN == "Krabi") %>% 
  select(ADM1_EN, date, gi_star)
```

We can now produce a plot by using the ggplot package.

```{r}
ggplot(data = kra, 
       aes(x = date, 
           y = gi_star)) +
  geom_line() +
  theme_light()

```
## Phuket

```{r}
phk<- gi_stars %>% 
  ungroup() %>% 
  filter(ADM1_EN == "Phuket") %>% 
  select(ADM1_EN, date, gi_star)
```

We now produce a plot to visualize this.

```{r}
ggplot(data = phk, 
       aes(x = date, 
           y = gi_star)) +
  geom_line() +
  theme_light()
```
## Chon Buri

```{r}
cbr<- gi_stars %>% 
  ungroup() %>% 
  filter(ADM1_EN == "Chon Buri") %>% 
  select(ADM1_EN, date, gi_star)
```
We now produce a plot for the same

```{r}
ggplot(data = cbr, 
       aes(x = date, 
           y = gi_star)) +
  geom_line() +
  theme_light()
```
## Chiang Mai

```{r}
chm<- gi_stars %>% 
  ungroup() %>% 
  filter(ADM1_EN == "Chiang Mai") %>% 
  select(ADM1_EN, date, gi_star)
```

We now create a plot for the same.

```{r}
ggplot(data = chm, 
       aes(x = date, 
           y = gi_star)) +
  geom_line() +
  theme_light()
```
:::

For all of these regions, besides Chiang Mai, we see a sharp drop between 2021 and 2022 however for the most part none of these regions reach their peaks between 2020 and 2022, which was when the world was hit by a global pandemic.

::: insights-box
Alternatively, we can also create an interactive plot using the ggplotly() function of the plotly package.

The code chunk below can be used.

```{r}
#| eval: false
p <- ggplot(data = cbg, 
       aes(x = date, 
           y = gi_star)) +
  geom_line() +
  theme_light()

ggplotly(p)
```
:::

We now generate the report for the Mann Kendall Test by implementing the below code chunk.

A Monotonic series or function is one that only increases (or decreases) and never changes direction. So long as the function either stays flat or continues to increase, it is monotonic.

-   **H0: No monotic trend.**

-   **H1: Monotonic trend is present**

::: insights-box
Tau ranges between -1 and 1 where:

-   -1 is a perfectly decreasing series.

-   1 is a perfectly increasing series.
:::

We implement the below code chunk to obtain the required report.

::: panel-tabset

## Bangkok

```{r}
bkk %>%
  summarise(mk = list(
    unclass(
      Kendall::MannKendall(gi_star)))) %>% 
  tidyr::unnest_wider(mk)
```

## Krabi

```{r}
kra %>%
  summarise(mk = list(
    unclass(
      Kendall::MannKendall(gi_star)))) %>% 
  tidyr::unnest_wider(mk)
```

## Chiang Mai 

```{r}
chm %>%
  summarise(mk = list(
    unclass(
      Kendall::MannKendall(gi_star)))) %>% 
  tidyr::unnest_wider(mk)
```

## Chon Buri 

```{r}
cbr %>%
  summarise(mk = list(
    unclass(
      Kendall::MannKendall(gi_star)))) %>% 
  tidyr::unnest_wider(mk)
```
 
## Phuket 

```{r}
phk %>%
  summarise(mk = list(
    unclass(
      Kendall::MannKendall(gi_star)))) %>% 
  tidyr::unnest_wider(mk)
```

:::

::: note-box
Note that `sl`in the output above is the p-value in this situation.
:::

##### 2.4.3.3.5 **Mann-Kendall Test Data-Frame**

We can perform the above steps for every location by using the group_by() function of the dplyr package.

```{r}
ehsa <- gi_stars %>%
  group_by(ADM1_EN) %>%
  summarise(mk = list(
    unclass(
      Kendall::MannKendall(gi_star)))) %>%
  tidyr::unnest_wider(mk)
head(ehsa)
```

We can sort the data-frame to highlight emerging hot/cold spots by implementing the below code chunk.

```{r}
emerging <- ehsa %>% 
  arrange(sl, abs(tau)) %>% 
  slice(1:10)
head(emerging)
```

##### 2.4.3.3.6 Emerging Hot-Spot Analysis

We now perform EHSA by using the [`emerging_hotspot_analysis()`](https://sfdep.josiahparry.com/reference/emerging_hotspot_analysis.html) function of the sfdep package.

It takes a spacetime object x (i.e: profit_st), and the quoted name of the variable of interest (i.e. GDPPC) as the .var argument.

The `k` argument is used to specify the number of time lags which is set to 1 by default.

`nsim` is number of simulations to be performed.

```{r}
ehsa <- emerging_hotspot_analysis(
  x = profits_st, 
  .var = "value", 
  k = 1, 
  nsim = 99
)
```

We now implement various ggplot2 functions to reveal the distributions of EHSA classes as a bar chart.

```{r}
ggplot(data = ehsa,
       aes(x = classification)) +
  geom_bar()
```

We can proceed with Visualizing EHSA.

```{r}
thai_ehsa <- province %>%
  left_join(ehsa,
            by = join_by(ADM1_EN == location))
```

We can now implement functions of the tmap package to produce a visualization for the above.

```{r}
ehsa_sig <- thai_ehsa  %>%
  filter(p_value < 0.1)
tmap_mode("plot")
tm_shape(thai_ehsa) +
  tm_polygons() +
  tm_borders(alpha = 0.5) +
tm_shape(ehsa_sig) +
  tm_fill("classification") + 
  tm_borders(alpha = 0.4)+
  tm_text('ADM1_EN', size=0.3)
```

We can further look at this in greater detail by each location as well if required.
